{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This course covers:\n",
    "\n",
    "* Intro to data science and machine learning\n",
    "* Problem formultation, data collection, exploratory data analysis\n",
    "* Data preprocessing and feature engineering\n",
    "* Model training, tuning, and debugging\n",
    "* Model evaluation, productionizing a ML model\n",
    "\n",
    "\n",
    "Ref: symbols via html entities see [here](https://html.spec.whatwg.org/entities.json). Markdown reference [here](https://github.github.com/gfm/#inlines)\n",
    "\n",
    "## What is Data Science?\n",
    "\n",
    "### Introduction to Data Science\n",
    "\n",
    "What is data science?\n",
    "\n",
    "* General definition: processes and systems to extract knowledge or insights from data, either structured or unstructured.\n",
    "* For the purposes of this course: managing, analyzing, and visualizing data in support of machine learning workflows.\n",
    "\n",
    "What is machine learning?\n",
    "\n",
    "* Artificial intelligence machines that improve their predictions by learning from large amounts of input data.\n",
    "\n",
    "Machine Learning\n",
    "\n",
    "* Main idea: learning equals estimating underlying function f by mapping data attributes to some target value\n",
    "* Training set: a set of labeled examples (x, f(x)) where x is the input variable and the label f(x) is the observed target truth\n",
    "* Goal: given a training set, find approximation f<sup>&Hat;</sup> of f that best generalizes, or predicts, labels for new examples.\n",
    "    * Best is measured by some quality measure\n",
    "    * Example: error rate, sum squared error\n",
    "\n",
    "Why Use Machine Learning\n",
    "\n",
    "* Some problems are too difficult to solve using conventional programming techniques\n",
    "    * Too complex (facial recognition)\n",
    "    * Too much data (stock market predictions)\n",
    "    * Information only available dynamically\n",
    "* Use of data for improvement\n",
    "    * Humans are used to improving based on experience\n",
    "* A lot of data is available\n",
    "    * Product recommendations\n",
    "    * Fraud detection\n",
    "    * Facial recognition\n",
    "    * Language understanding\n",
    "\n",
    "Types of machine learning algoritms\n",
    "\n",
    "* Supervised learning - target variable used to determine the truth. Human intelligence baked in with the labeling of the training data.\n",
    "* Unsupervised learning - only a collection of features with no specific outcome, no human in the loop\n",
    "* Semi-supervised learning - combination of supervised and unsuperised learning, used in cases where you have a mixture of labeled and unlabeled data.\n",
    "* Reinforcement learning - you use rewards and penalties to let the algoritm learn, reward outcomes and behaviors you want to encourage.\n",
    "\n",
    "Data Matters\n",
    "\n",
    "* The more data, the better the accuracy\n",
    "* High quality training data set is the essential component for success\n",
    "\n",
    "Data Science Workflow\n",
    "\n",
    "* Phase 1 - Problem Formation\n",
    "* Phase 2\n",
    "    * Data Collectopm\n",
    "    * Exploratory Analysis\n",
    "    * Data Preprocessing\n",
    "    * Feature Engineering\n",
    "* Phase 3\n",
    "    * Model training\n",
    "    * Model evaluation\n",
    "    * Model tuning\n",
    "    * Model debugging\n",
    "* Phase 4\n",
    "    * Productionisation\n",
    "\n",
    "Important Concepts\n",
    "\n",
    "* The data set - partition, don't use the same data in training and model validation\n",
    "* The validation set should be targeted towards business requirements, but include enough outliers to give a realistic view of the model's performance.\n",
    "* Feature = attribute = independent variable = predictor\n",
    "* Label = target = outcome = class = dependent variable = response\n",
    "* Dimensionality = number of features\n",
    "* Model selection\n",
    "\n",
    "\n",
    "### Types of Machine Learning\n",
    "\n",
    "Supervised Learning\n",
    "\n",
    "* Learning with feedback provided in the learning data\n",
    "* Each training example is provided with the correct label\n",
    "* Regression - target type is a continuous value\n",
    "* Classification - target type is catagorical\n",
    "\n",
    "Unsupervised Learning\n",
    "\n",
    "* No target column/outcode\n",
    "* Have a collection of features/attributes from each observation\n",
    "* Grouping/clustering for downstream analysis\n",
    "\n",
    "Reinforcement Learning\n",
    "\n",
    "* Algorithm is not told what action to take, but is given a reward or penalty for after each action in a sequence\n",
    "* Example - teach a machine how to play video games\n",
    "\n",
    "### Key Issues in Machine Learning\n",
    "\n",
    "Data Quality\n",
    "\n",
    "* High quality data is the secret sauce\n",
    "* Quality\n",
    "    * Consistency of the data - consider the business problem; is the data we're using consistent with the problem we are trying to solve\n",
    "    * Accuracy of the data - labels, features (numerical and categorical)\n",
    "    * Noisy data - many input and output fluctuations\n",
    "    * Missing data - algoritms can't deal with missing data\n",
    "    * Outliers - errors, typos, correct but not relevant or out of scope\n",
    "    * Bias\n",
    "    * Variance\n",
    "\n",
    "\n",
    "Model Quality\n",
    "\n",
    "* Underfitting vs overfitting\n",
    "* Overfitting\n",
    "    * Failure to generalize - performs well on training data but poorly on test\n",
    "    * Can indicate model is too flexible\n",
    "    * Flexible - allows it to memorize the data including the noise\n",
    "    * Corresponds to high variance - small changes in the training data leads to big changes in the results\n",
    "* Underfitting\n",
    "    * failure to capture important patterns in the training data set\n",
    "    * Typically indicates model is too simple or there are too few explanitory variables\n",
    "    * Not flexible enough to model real patterns\n",
    "    * Corresponds to high bias - the results show systematic lack of fit in certain regions\n",
    "\n",
    "Computation Speed and Scalability\n",
    "\n",
    "* Use distributed computing systems like Sage Maker of EC2 instances for training in order to:\n",
    "    * Increase speed\n",
    "    * Solve prediction time complexity\n",
    "    * Solve space complexity\n",
    "* May need to address latency and scalability instances\n",
    "\n",
    "### Supervised Methods: Linear Regression\n",
    "\n",
    "#### Linear regression\n",
    "\n",
    "Linear methods\n",
    "\n",
    "* Parametric methods where function learned has form f(x) = &Phi;(w<sup>T</sup>x) where &Phi;() is some activation function.\n",
    "* Generally optimized by learning weights by applying (stochastic) gradient descent to minimize loss function e.g. &Sigma; |y<sup>&Hat;</sup><sub>i</sub> - y<sub>i</sub>|<sup>2</sup>\n",
    "* Simple; a good place to start for a new problem, at least as a baseline\n",
    "* Methods\n",
    "    * linear regression for numeric target outcome\n",
    "    * logistic regression for categorical target outcome\n",
    "\n",
    "Univariate linear regression\n",
    "\n",
    "* Model relation between a single feature (explanatory variable x) and a real-valued response (target variable y), for example area as price predictor for real estate price\n",
    "* Error is |y<sup>&Hat;</sup><sub>i</sub> - y<sub>i</sub>|, e.g. predicted price minus real price.\n",
    "* Given data (x,y) and a line defined by w<sub>0</sub> (intercept) and slope w<sub>1</sub> (slope), the vertical offset for each data point from the line is the error between the true label y and the prediction based on x\n",
    "* The best line minimizes the sum of squared errors (SSE)\n",
    "* We usually assume the error is Guassian distributed with mean zero and fixed variance\n",
    "\n",
    "Multivariable Linear Regression\n",
    "\n",
    "* Multiple linear regression includes N explanitory variables with N >= 2\n",
    "\n",
    "$$y = w_0 x_0 + w_1 x_1 + ... + w_m x_m = \\sum_{i=0}^N w_i X_i$$\n",
    "\n",
    "* Sensitive to correlation between features, resulting in high variance of coefficients, can lead to multi-colinearality\n",
    "* scikit-learn implementation: sklearn.linear_model.LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: Linear Regression and Linear Separability\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "* Response variable is binary (yes/no, 1/0, true/false, positive/negative)'\n",
    "\n",
    "Example:\n",
    "\n",
    "* Credit card transaction - predict fraud, response is yes or no.\n",
    "* 1 fradulant, 0 legit\n",
    "* Lots of features gone into the forecast, but two states in the response\n",
    "* How to connect the features x to the binary response.\n",
    "* Can use the sigmoid function to quantify the status of the input in between the two outputs.\n",
    "\n",
    "$$ \\sigma (z) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-z} }  $$ \n",
    "\n",
    "* z is a trained multivariate linear function\n",
    "* $\\phi$ is a fixed univariate function (not trained)\n",
    "* Objective function to maximize = probability of the true training labels\n",
    "* Sigmoid curve - good way to represent probability, returns between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
