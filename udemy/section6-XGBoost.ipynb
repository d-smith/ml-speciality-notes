{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Gradient Boosted Trees\n",
    "\n",
    "## Algorithms Overview\n",
    "\n",
    "XGBoost read the docs site [here](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n",
    "\n",
    "Linear model\n",
    "\n",
    "* Pros\n",
    "    * Simple and easy to understand\n",
    "    * Performs suprisingly well for a variety of problems\n",
    "* Cons\n",
    "    * Difficulty handling non-linear datasets\n",
    "    * Features need to be on a similar scale, categorical values need to be one-hot encoded, complex feature engineering may be required\n",
    "    \n",
    "Decision Tree\n",
    "\n",
    "* Separates data into different groups using a series of questions\n",
    "* During training the algorithm selects the questions to build the tree\n",
    "* For decisions the algoritm walks the tree and returns the class of the leaf node\n",
    "\n",
    "* Pros\n",
    "    * Handle non-linear datasets\n",
    "    * Can handle feature on different scales\n",
    "* Cons\n",
    "    * Can overfit if the tree gets too deep. Limiting the depth of the tree can avoid overfitting, but this can cause underfitting as the tree may not learn the patterns.\n",
    "    \n",
    "Ensemble Methods\n",
    "   \n",
    "* Use multiple trees and combines them to achieve better results\n",
    "* Two methods used to come up with the set of decision trees: bagging and boosting\n",
    "\n",
    "* Bagging\n",
    "    * Training algorithm uses a random sample of training data at each step to form a tree\n",
    "    * Can sample features, observations, or a combination of these\n",
    "* Boosting\n",
    "    * Algoritm starts with a simple tree\n",
    "    * Tree is evaluated - some predictions from that tree will be right, some wrong\n",
    "    * Incorrect predictions are given a higher weight\n",
    "    * Next a second tree is built focused on the incorrect predictions.  The process is then repeated until there are no more improvements or a limit on the number of trees to create has been reached\n",
    "    \n",
    "    \n",
    "* Pros\n",
    "    * Not very sensitive to data distribution\n",
    "    * Can easily handle features on different scales\n",
    "    * Handles categorical data without one hot encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab\n",
    "\n",
    "* [linear regression data prep](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/linear_data_preparation.ipynb)\n",
    "\n",
    "Once the data has been prepared, used [this](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/linear_xgboost_localmode.ipynb) notebook.\n",
    "\n",
    "Faster way to install xgboost:\n",
    "\n",
    "```console\n",
    "!pip install xgboost==0.90\n",
    "```\n",
    "\n",
    "Tree based algorithms makes branch based decisions based on data seen in training. Linear regression captures the relationship between inputs and output using weights, which means it can extrapoloate. Conversely, tree based methods have limits in terms of the range of data they can make predictions using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab\n",
    "\n",
    "* [Non-leaner data prep](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/quadratic_data_preparation.ipynb)\n",
    "\n",
    "* [Quadratic regression dataset - linear regression vs xgboost](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/quadratic_xgboost_localmode.ipynb)\n",
    "\n",
    "Lab solution: add a quadratic feature to model the, um... quadratic equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab - Bike Sharing Kaggle Challenge \n",
    "\n",
    "Forecast hourly demand\n",
    "\n",
    "* Kaggle info [here](https://www.kaggle.com/c/bike-sharing-demand/data)\n",
    "* How to [download datasets from kaggle](https://freddiek.github.io/2018/06/10/accessing-Kaggle-from-SageMaker-instance.html)\n",
    "\n",
    "Download the data using the kaggle command, e.g.\n",
    "\n",
    "```console\n",
    "kaggle competitions download -c bike-sharing-demand\n",
    "```\n",
    "\n",
    "Data prep - rev1 workbook\n",
    "\n",
    "* xgboost can only handle numerical features and categorical values\n",
    "* need to break up the time stamp into year, month, day, day of week, hour\n",
    "* workbook shows several ways to explore the dataset\n",
    "* For this lab we have a single model to predict total rentals; could have made two different models to predict casual and registered rentals, then add them up.\n",
    "\n",
    "Train regression model\n",
    "\n",
    "* bike rental xgboost localmode rev1\n",
    "* hyper parameters - depth of 5, 150 trees max\n",
    "* Can examine feature importance - hour and humidity are most influential\n",
    "* Something funky - regression model predicting negative rentals\n",
    "    * For plotting purposes set the negative predictions to 0\n",
    "* Kaggle uses RMSLE - root mean square log error - the % of difference matters, not the magnitude of the difference\n",
    "\n",
    "Optimization Technique\n",
    "\n",
    "* When you model needs to predict a positive integer like count, you can apply a log transformation on the target, e.g. log(count)\n",
    "* To get the predicted count, use an inverse transform on the predicted value, e.g. exp(count)\n",
    "* Smoothens the effect of seasonality and trend, brings count to a similar scale\n",
    "* See notebook bike rental data prep rev 3\n",
    "* And bikerental xgboost localmode rev3\n",
    "* This optimization performs much better than rev 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Model Using SageMaker's XGBoost\n",
    "\n",
    "Four steps:\n",
    "\n",
    "* Upload training and validation files to S3\n",
    "* Specify algorithm and hyperparameters\n",
    "* Configure type of server and number of servers to use for training\n",
    "* Create a real-time endpoint for interactive use case\n",
    "\n",
    "Lab:\n",
    "\n",
    "* Open xgboost cloud training template notebook for the lab\n",
    "* Note get image uri now returns the image location in ECR\n",
    "* You can see the training job running in the sage maker console\n",
    "\n",
    "How to Connect to an Existing SageMaker Endpoint\n",
    "\n",
    "* Use the [cloud prediction notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/BikeSharingRegression/xgboost_cloud_prediction_template.ipynb)\n",
    "* Prediction endpoints accept batches of values and can return an array of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hosting\n",
    "\n",
    "Single Instance Hosting - single point of failure.\n",
    "\n",
    "* SageMaker integrates with CloudWatch\n",
    "* SageMaker can also be integrated with autolaunch\n",
    "* Configure an endpoint with multiple instances, so requests are distributed against multiple instances in multiple AZs for HA\n",
    "* Scale based on workload as well\n",
    "* SageMakerVariantInvocationsPerInstance metric = average number of requests per minute per instance\n",
    "\n",
    "Multiple Instances (Variants) of an Algorithm can be deployed to the same endpoint\n",
    "\n",
    "* Good for testing new versions of models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
