{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Gradient Boosted Trees\n",
    "\n",
    "## Algorithms Overview\n",
    "\n",
    "XGBoost read the docs site [here](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n",
    "\n",
    "Linear model\n",
    "\n",
    "* Pros\n",
    "    * Simple and easy to understand\n",
    "    * Performs suprisingly well for a variety of problems\n",
    "* Cons\n",
    "    * Difficulty handling non-linear datasets\n",
    "    * Features need to be on a similar scale, categorical values need to be one-hot encoded, complex feature engineering may be required\n",
    "    \n",
    "Decision Tree\n",
    "\n",
    "* Separates data into different groups using a series of questions\n",
    "* During training the algorithm selects the questions to build the tree\n",
    "* For decisions the algoritm walks the tree and returns the class of the leaf node\n",
    "\n",
    "* Pros\n",
    "    * Handle non-linear datasets\n",
    "    * Can handle feature on different scales\n",
    "* Cons\n",
    "    * Can overfit if the tree gets too deep. Limiting the depth of the tree can avoid overfitting, but this can cause underfitting as the tree may not learn the patterns.\n",
    "    \n",
    "Ensemble Methods\n",
    "   \n",
    "* Use multiple trees and combines them to achieve better results\n",
    "* Two methods used to come up with the set of decision trees: bagging and boosting\n",
    "\n",
    "* Bagging\n",
    "    * Training algorithm uses a random sample of training data at each step to form a tree\n",
    "    * Can sample features, observations, or a combination of these\n",
    "* Boosting\n",
    "    * Algoritm starts with a simple tree\n",
    "    * Tree is evaluated - some predictions from that tree will be right, some wrong\n",
    "    * Incorrect predictions are given a higher weight\n",
    "    * Next a second tree is built focused on the incorrect predictions.  The process is then repeated until there are no more improvements or a limit on the number of trees to create has been reached\n",
    "    \n",
    "    \n",
    "* Pros\n",
    "    * Not very sensitive to data distribution\n",
    "    * Can easily handle features on different scales\n",
    "    * Handles categorical data without one hot encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab\n",
    "\n",
    "* [linear regression data prep](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/linear_data_preparation.ipynb)\n",
    "\n",
    "Once the data has been prepared, used [this](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/linear_xgboost_localmode.ipynb) notebook.\n",
    "\n",
    "Faster way to install xgboost:\n",
    "\n",
    "```console\n",
    "!pip install xgboost==0.90\n",
    "```\n",
    "\n",
    "Tree based algorithms makes branch based decisions based on data seen in training. Linear regression captures the relationship between inputs and output using weights, which means it can extrapoloate. Conversely, tree based methods have limits in terms of the range of data they can make predictions using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab\n",
    "\n",
    "* [Non-leaner data prep](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/quadratic_data_preparation.ipynb)\n",
    "\n",
    "* [Quadratic regression dataset - linear regression vs xgboost](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/quadratic_xgboost_localmode.ipynb)\n",
    "\n",
    "Lab solution: add a quadratic feature to model the, um... quadratic equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab - Bike Sharing Kaggle Challenge \n",
    "\n",
    "Forecast hourly demand\n",
    "\n",
    "* Kaggle info [here](https://www.kaggle.com/c/bike-sharing-demand/data)\n",
    "* How to [download datasets from kaggle](https://freddiek.github.io/2018/06/10/accessing-Kaggle-from-SageMaker-instance.html)\n",
    "\n",
    "Download the data using the kaggle command, e.g.\n",
    "\n",
    "```console\n",
    "kaggle competitions download -c bike-sharing-demand\n",
    "```\n",
    "\n",
    "Data prep - rev1 workbook\n",
    "\n",
    "* xgboost can only handle numerical features and categorical values\n",
    "* need to break up the time stamp into year, month, day, day of week, hour\n",
    "* workbook shows several ways to explore the dataset\n",
    "* For this lab we have a single model to predict total rentals; could have made two different models to predict casual and registered rentals, then add them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
