{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Gradient Boosted Trees\n",
    "\n",
    "## Algorithms Overview\n",
    "\n",
    "XGBoost read the docs site [here](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n",
    "\n",
    "Linear model\n",
    "\n",
    "* Pros\n",
    "    * Simple and easy to understand\n",
    "    * Performs suprisingly well for a variety of problems\n",
    "* Cons\n",
    "    * Difficulty handling non-linear datasets\n",
    "    * Features need to be on a similar scale, categorical values need to be one-hot encoded, complex feature engineering may be required\n",
    "    \n",
    "Decision Tree\n",
    "\n",
    "* Separates data into different groups using a series of questions\n",
    "* During training the algorithm selects the questions to build the tree\n",
    "* For decisions the algoritm walks the tree and returns the class of the leaf node\n",
    "\n",
    "* Pros\n",
    "    * Handle non-linear datasets\n",
    "    * Can handle feature on different scales\n",
    "* Cons\n",
    "    * Can overfit if the tree gets too deep. Limiting the depth of the tree can avoid overfitting, but this can cause underfitting as the tree may not learn the patterns.\n",
    "    \n",
    "Ensemble Methods\n",
    "   \n",
    "* Use multiple trees and combines them to achieve better results\n",
    "* Two methods used to come up with the set of decision trees: bagging and boosting\n",
    "\n",
    "* Bagging\n",
    "    * Training algorithm uses a random sample of training data at each step to form a tree\n",
    "    * Can sample features, observations, or a combination of these\n",
    "* Boosting\n",
    "    * Algoritm starts with a simple tree\n",
    "    * Tree is evaluated - some predictions from that tree will be right, some wrong\n",
    "    * Incorrect predictions are given a higher weight\n",
    "    * Next a second tree is built focused on the incorrect predictions.  The process is then repeated until there are no more improvements or a limit on the number of trees to create has been reached\n",
    "    \n",
    "    \n",
    "* Pros\n",
    "    * Not very sensitive to data distribution\n",
    "    * Can easily handle features on different scales\n",
    "    * Handles categorical data without one hot encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab\n",
    "\n",
    "* [linear regression data prep](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/linear_data_preparation.ipynb)\n",
    "\n",
    "Once the data has been prepared, used [this](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/linear_xgboost_localmode.ipynb) notebook.\n",
    "\n",
    "Faster way to install xgboost:\n",
    "\n",
    "```console\n",
    "!pip install xgboost==0.90\n",
    "```\n",
    "\n",
    "Tree based algorithms makes branch based decisions based on data seen in training. Linear regression captures the relationship between inputs and output using weights, which means it can extrapoloate. Conversely, tree based methods have limits in terms of the range of data they can make predictions using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab\n",
    "\n",
    "* [Non-leaner data prep](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/quadratic_data_preparation.ipynb)\n",
    "\n",
    "* [Quadratic regression dataset - linear regression vs xgboost](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/LinearAndQuadraticFunctionRegression/quadratic_xgboost_localmode.ipynb)\n",
    "\n",
    "Lab solution: add a quadratic feature to model the, um... quadratic equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab - Bike Sharing Kaggle Challenge \n",
    "\n",
    "Forecast hourly demand\n",
    "\n",
    "* Kaggle info [here](https://www.kaggle.com/c/bike-sharing-demand/data)\n",
    "* How to [download datasets from kaggle](https://freddiek.github.io/2018/06/10/accessing-Kaggle-from-SageMaker-instance.html)\n",
    "\n",
    "Download the data using the kaggle command, e.g.\n",
    "\n",
    "```console\n",
    "kaggle competitions download -c bike-sharing-demand\n",
    "```\n",
    "\n",
    "Data prep - rev1 workbook\n",
    "\n",
    "* xgboost can only handle numerical features and categorical values\n",
    "* need to break up the time stamp into year, month, day, day of week, hour\n",
    "* workbook shows several ways to explore the dataset\n",
    "* For this lab we have a single model to predict total rentals; could have made two different models to predict casual and registered rentals, then add them up.\n",
    "\n",
    "Train regression model\n",
    "\n",
    "* bike rental xgboost localmode rev1\n",
    "* hyper parameters - depth of 5, 150 trees max\n",
    "* Can examine feature importance - hour and humidity are most influential\n",
    "* Something funky - regression model predicting negative rentals\n",
    "    * For plotting purposes set the negative predictions to 0\n",
    "* Kaggle uses RMSLE - root mean square log error - the % of difference matters, not the magnitude of the difference\n",
    "\n",
    "Optimization Technique\n",
    "\n",
    "* When you model needs to predict a positive integer like count, you can apply a log transformation on the target, e.g. log(count)\n",
    "* To get the predicted count, use an inverse transform on the predicted value, e.g. exp(count)\n",
    "* Smoothens the effect of seasonality and trend, brings count to a similar scale\n",
    "* See notebook bike rental data prep rev 3\n",
    "* And bikerental xgboost localmode rev3\n",
    "* This optimization performs much better than rev 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Model Using SageMaker's XGBoost\n",
    "\n",
    "Four steps:\n",
    "\n",
    "* Upload training and validation files to S3\n",
    "* Specify algorithm and hyperparameters\n",
    "* Configure type of server and number of servers to use for training\n",
    "* Create a real-time endpoint for interactive use case\n",
    "\n",
    "Lab:\n",
    "\n",
    "* Open xgboost cloud training template notebook for the lab\n",
    "* Note get image uri now returns the image location in ECR\n",
    "* You can see the training job running in the sage maker console\n",
    "\n",
    "How to Connect to an Existing SageMaker Endpoint\n",
    "\n",
    "* Use the [cloud prediction notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/BikeSharingRegression/xgboost_cloud_prediction_template.ipynb)\n",
    "* Prediction endpoints accept batches of values and can return an array of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hosting\n",
    "\n",
    "Single Instance Hosting - single point of failure.\n",
    "\n",
    "* SageMaker integrates with CloudWatch\n",
    "* SageMaker can also be integrated with autolaunch\n",
    "* Configure an endpoint with multiple instances, so requests are distributed against multiple instances in multiple AZs for HA\n",
    "* Scale based on workload as well\n",
    "* SageMakerVariantInvocationsPerInstance metric = average number of requests per minute per instance\n",
    "\n",
    "Multiple Instances (Variants) of an Algorithm can be deployed to the same endpoint\n",
    "\n",
    "* Good for testing new versions of models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "Lab\n",
    "\n",
    "* Iris Data Classification - [Data Prep](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/IrisClassification/iris_data_preparation.ipynb)\n",
    "    * String class labels must be encoded as integers\n",
    "    * Use sklearn label encoder\n",
    "* Iris Classification - [Train the Model](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/IrisClassification/iris_xgboost_localmode.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "Lab\n",
    "\n",
    "* Diabetes Data Classification - [Data Prep](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/DiabetesClassification/diabetes_data_preparation.ipynb)\n",
    "* Diabetes Classification - [Model Training](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/xgboost/DiabetesClassification/diabetes_xgboost_localmode.ipynb)\n",
    "\n",
    "Performance of the original data was poor, but probably related to some of the problems with the data, such as the zero values for some data that cannot be data. Debug the data before debugging the model.\n",
    "\n",
    "Solution: replace the missing values with the mean of the non-missing values. Note that the 'missing values' are actually numbers, so be sure that are not included in the calculation of the means. One way to do this io replace them withnp.nan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "* Fine tune the learning process based on the complexity of the data set.\n",
    "* Algorithms usually use a sensible set of default values.\n",
    "\n",
    "objective hyperparameter - specifies the learning task and the corresponding learning objective\n",
    "\n",
    "* regression - \"reg:linear\"\n",
    "* binary classification - \"binary:logistic\"\n",
    "* multiclass classification - \"multi:softmax\"\n",
    "\n",
    "Other important parameters:\n",
    "\n",
    "* num_rounds - number of trees, too high can overfit\n",
    "* early_stopping_rounds - stop the training when the validation score stops improving, helps avoid overfitting\n",
    "\n",
    "Bias and Variance\n",
    "\n",
    "* Biased models - do not match reality\n",
    "* Variance\n",
    "    * How well the model generalizes for unseen data.\n",
    "    * Difference between validation error and training error\n",
    "\n",
    "High Bias\n",
    "\n",
    "* Data is not learning from data\n",
    "* Translates to large training and validation errors\n",
    "* Underfitting\n",
    "\n",
    "High Variance\n",
    "\n",
    "* Validation error is high, but training error is low.\n",
    "* Overfitting\n",
    "\n",
    "Handling High Bias\n",
    "\n",
    "* Add relevant features\n",
    "* Combine features\n",
    "* Create higher order features\n",
    "* Train longer (more iterations)\n",
    "* Decrease regularization\n",
    "\n",
    "Handling High Variance\n",
    "\n",
    "* Use fewer features\n",
    "* Use straightforward features instead of higher order features\n",
    "* Reduce training iterations\n",
    "* Increase regularization\n",
    "\n",
    "Regularization\n",
    "\n",
    "* Many features are equally good at predicting outcome\n",
    "* Which combination of features is the model going to use?\n",
    "* Feature selection depends on algorithm and regularization parameters.\n",
    "\n",
    "Regularization tones down overdependence on specific features/ \n",
    "\n",
    "Analogy: google maps on the phone vs garmin gps vs paper map\n",
    "\n",
    "* Google maps has most up to date context (road closure, traffic, accidents), but has many failure scenarios that can leave you with no information.\n",
    "* GPS lacks most up to date road closures, traffic conditions, etc. but is less prone to failures - balances risk against performance\n",
    "\n",
    "L1 Regularization - Aggresively eliminates features that are not important\n",
    "\n",
    "* Example:\n",
    "    * Phone GPS - substantial weight\n",
    "    * Standalone GPS - zero weight\n",
    "    * Paper map - zero weight\n",
    "* Useful in large dimension dataset - reduce the number of features\n",
    "\n",
    "L2 Regularization - Simply reduces weight of some features\n",
    "\n",
    "* Allows other features to influence outcome\n",
    "* L2 regularization is a good starting point\n",
    "* Example:\n",
    "    * Phone GPS - larger weight\n",
    "    * Standalone GPS - medium weight\n",
    "    * Paper map - smaller weight\n",
    "    \n",
    "XGBoost Regularization\n",
    "\n",
    "* alpha - L1 regularization, default is 0. \n",
    "* lambda - L0 regularization, default it 1.\n",
    "* XGBoost tuning guide - see [here](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html)\n",
    "\n",
    "Hyperparameter tuning is dependent on the dataset, and some hyper parameters are sensitive to the settings of other hyperparameters. There it is recommended to use automated tuning methods.\n",
    "\n",
    "SKLearn Automatic Tuning\n",
    "\n",
    "* GridSearch - exhaustive search using specified lower and upper bound of parameter values\n",
    "* RandomSearch - random search of parameters from specified lower and upper bound\n",
    "\n",
    "SageMaker - Automatic Tuning\n",
    "\n",
    "* Bayesian Search - smart search. Treats hyperparameter tuning as a machine learning problem. Often converges faster.\n",
    "* Random search - random search of parameters from specified lower and upper bound, similar to SKLearn random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
