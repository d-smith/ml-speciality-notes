{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters are tunable parameters of machine learning algorithms\n",
    "\n",
    "* XGBoost max_depth - depth of trees in XGBoost, tune overfitting vs underfitting\n",
    "* Epochs - number of passes of the data\n",
    "* Learning_rate - may need more passes to learn if small (related to epochs)\n",
    "* Good to start with defaults, then tune\n",
    "\n",
    "Hyperparameter Tuning is Hard\n",
    "\n",
    "* Time consuming\n",
    "* Algorithms can have several parameters with a wide range of value\n",
    "* Interdependencies between hyperparameters\n",
    "* Need a systematic way to tune\n",
    "\n",
    "SageMaker Automatic Model Training\n",
    "\n",
    "* Input: \n",
    "    * Algorithm, training and test data\n",
    "    * Hyperparameters and range of values to search\n",
    "    * Optimization objective (e.g. rmse, binary classification accuracy, etc)\n",
    "* Tuning\n",
    "    * Launch multiple training jobs - with different hyperparameter values\n",
    "    * Pick the training job and hyperparameters that offer best performance\n",
    "    \n",
    "Search Strategy\n",
    "\n",
    "* Random search\n",
    "* Bayesian\n",
    "    * Tuning itself treated like a regression problem\n",
    "    * Input features: hyperparameters\n",
    "    * Target: optimization objective\n",
    "    * Process:\n",
    "        * Start with some combination of hyperparameters\n",
    "        * Runs training jobs with these values\n",
    "        * Access outcome\n",
    "        * Use regression to choose next set of hyperparameters\n",
    "        * Repeat\n",
    "        \n",
    "agemaker hyperparameter tuning job is stochastic in nature. and when you run a tuning job with the same input parameters, you may see slightly different results.\n",
    "        \n",
    "Lab: Tune Factorization Machine\n",
    "\n",
    "* Recommender system to predict how a user would rate movies\n",
    "* Test RMSE score degraded from o.8 to 1.9 with new movie data set\n",
    "* Factorization hyperparameters - difficult to understand\n",
    "* Optimize as black box\n",
    "\n",
    "Data prep - see [this notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/fm/movie_data_preparation.ipynb)\n",
    "\n",
    "Training - see [this notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/fm/fm_cloud_training_template.ipynb)\n",
    "\n",
    "From the console\n",
    "\n",
    "* Training > Hyperparameter tuning jobs\n",
    "* New job, name it\n",
    "* Add permissions to access s3, run training jobs.\n",
    "* Select built in alg, factorization machines.\n",
    "* Strategy - pick Bayesian\n",
    "* Learning objective - pick test:rmse\n",
    "* Configure hyperparameter search space as per lecture\n",
    "\n",
    "\n",
    "Next, use the hyperparameters from the first tuning to refine the search.\n",
    "\n",
    "After the refinement of the hyperparameters, deploy the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
