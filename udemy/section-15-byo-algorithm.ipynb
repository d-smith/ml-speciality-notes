{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring Your Own Algoritm\n",
    "\n",
    "\n",
    "SageMaker Training and Hosting \n",
    "\n",
    "* Built-in algorithms\n",
    "* Pre-built container images\n",
    "    * Supports popular frameworks like MxNet, TensorFlow\n",
    "    * FLexibility to use a wide selection of algoritms\n",
    "* Extend pre-built container images\n",
    "* Custom container images - use a different language and framework\n",
    "\n",
    "Last 3 options allow you to bring your own algorithm\n",
    "\n",
    "\n",
    "## Built-In Algorithms\n",
    "\n",
    "### Training\n",
    "\n",
    "Built-in algorithms\n",
    "\n",
    "* Images stored in a public ECR\n",
    "* Data needed to train and test the model are stored in an S3 model\n",
    "* SageMaker spins up the training instances, downloads the container and hyper parameters, downloads the traing and test data.\n",
    "* Model is copied to s3 then training instances are terminated.\n",
    "\n",
    "\n",
    "### Hosting\n",
    "\n",
    "* Supply hosting configuration:\n",
    "    * Image to use \n",
    "    * Model in s3 to host\n",
    "    * Instance type and number of instances\n",
    "* SageMaker...\n",
    "    * Spins up the hosting instances\n",
    "    * Downloads the model\n",
    "    * Uses the hosting entry point of the container\n",
    "    \n",
    "## Custom Image\n",
    "\n",
    "* Package your algorithm in a container that meets SageMaker standards/conventions\n",
    "* Store the image in ECR\n",
    "\n",
    "From here the training and hosting is almost identical to prebuilt\n",
    "\n",
    "* Container must also specify service and training entry points\n",
    "* When training is done must serialize model artifacts to a local directory\n",
    "\n",
    "\n",
    "## Popular Frameworks\n",
    "\n",
    "SageMaker provides framework images for popular frameworks - SKLearn, Tensorflow, MxNet, PyTorch, etc.\n",
    "\n",
    "* Write script to define algoritm you want to use or even define a new algoritm.\n",
    "* Container then uses the script during the fit step.\n",
    "* Script uses hyperparameters and local data to train the model, then writes the fitted model to a local folder\n",
    "\n",
    "You can also use local mode - helps with script writing, integration, etc.\n",
    "\n",
    "Hosting\n",
    "\n",
    "* Use same image, provide location to model in s3\n",
    "* Might need a script file depending on framework\n",
    "\n",
    "## Folder Structure and Env Variables\n",
    "\n",
    "* Standard structure for reading data and resources\n",
    "* Entry point that contains the code to run when the container is started\n",
    "* Instrumentation - uses StdOut, StdErr - metrics are sent to CloudWatch\n",
    "* Metric Capture - log metrics and define regex patterns to capture values from log\n",
    "* One image for training and hosting or different images (when computer resource requirements substantially different)\n",
    "\n",
    "\n",
    "Folder Structure\n",
    "\n",
    "* /opt/ml/input - contains hyperparameters and data needed for training \n",
    "    * /config\n",
    "    * /data\n",
    "        * /channel\n",
    "* /opt/ml/code - scripts used for training and serving\n",
    "* /opt/ml/model - stores trained model\n",
    "* /opt/ml/output - errors that happened during training\n",
    "    * /failure\n",
    "    \n",
    "Details - Training Input\n",
    "\n",
    "* /opt/ml/input/config/hyperparameters.json - hyper params for training\n",
    "* /opt/ml/input/config/resourceConfig.json - container network layout for training\n",
    "* /opt/ml/input/data/channel/ - channel = training, testing. Containers files for each channel, e.g. /opt/ml/input/data/training, /opt/ml/input/data/testing\n",
    "* /opt/ml/input/data/channel_epoch/ - channel = training, test, eval, etc. Epoch = 0,1,2... creates a named pipe for channel_epic, read the pipe to stream data from s3 for each epoch\n",
    "* /opt/ml/code - scripts to run from the container\n",
    "\n",
    "\n",
    "Details - Training Output\n",
    "\n",
    "* /opt/ml/model/ - script should write generated model to this directory, store model checkpoints and final output, sagemaker uploads the content of model folder to your s3 bucket\n",
    "* /opt/ml/output/failure - if training fails write the error description to the failure file. Sagemaker returns the first 1024 chars as the failure reason in the job descripton. SageMaker uploads content of output folder to s3\n",
    "\n",
    "Details - Hosting\n",
    "\n",
    "* /opt/ml/model - model files to use for inference\n",
    "* /opt/ml/code - scripts to run from container\n",
    "\n",
    "Environment Variables\n",
    "\n",
    "* See https://github.com/aws/sagemaker-containers#how-a-script-is-executed-inside-the-container\n",
    "* Can get hyperparams as environment variables and as script arguments\n",
    "\n",
    "How a script is executed - see [here](https://github.com/aws/sagemaker-containers#how-a-script-is-executed-inside-the-container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab - SKLearn Estimator Bring Your Own\n",
    "\n",
    "Start with local mode, then switch to cloud training and hosting.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Need ml.m5.xlarge\n",
    "* Files from [this repo](https://github.com/ChandraLingam/AmazonSageMakerCourse/tree/master/CustomAlgorithm/ScikitLearn/Iris)\n",
    "    * daemon and setup scripts provided by amazon\n",
    "    * data prep from the other iris labs\n",
    "    * skikit_learn_iris.py is the script\n",
    "    * iris_scikit_learn_training_and_serving.ipynb is the file that controls learning and serving\n",
    "    \n",
    "Toubleshooting\n",
    "\n",
    "* Choose local for instance type\n",
    "* Set instance type to desired EC2 type when doing cloud training\n",
    "\n",
    "Misc\n",
    "\n",
    "* Health check - pinged by runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab - TensorFlow Estimator Bring Your Own\n",
    "\n",
    "Notes:\n",
    "\n",
    "* [This repo](https://github.com/ChandraLingam/AmazonSageMakerCourse/tree/master/CustomAlgorithm/TensorFlow/Iris)\n",
    "* [Main script](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/CustomAlgorithm/TensorFlow/Iris/iris_tensorflow_training_and_serving.ipynb) shows changes from iris NN lab\n",
    "* Loss function is sparse_categorical_crossentropy - does not require one hot encoding of labels (categorical_crossentropy used earlier does...)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
