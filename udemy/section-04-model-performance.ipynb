{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance\n",
    "\n",
    "Terminology\n",
    "\n",
    "* Labeled data - split into *training set* and *test set*\n",
    "\n",
    "Model fit - see [here](https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html)\n",
    "\n",
    "* Underfitting - performs poorly on training and test set. \n",
    "    * Model fails to capture relationship between inputs and output.\n",
    "    * Fix by adding features, adding more complex features, adding more examples to the training set, optimize hyper parameters\n",
    "* Overfitting - performs well on the training data, and poorly on the test data\n",
    "    * Model 'memorizes' the training data, and fails to generalize to unseen data\n",
    "    * Correct by removing more complex features,optimize hyper parameters\n",
    "    \n",
    "Supervised Learning Algorithm Types\n",
    "\n",
    "* Regression - output is continuous numeric\n",
    "* Binary classification - output is binary\n",
    "* Multi-class classification - Categorical - one of many possible outcomes\n",
    "\n",
    "## Regression Model Performance\n",
    "\n",
    "Common techniques for evaluating model performance:\n",
    "\n",
    "* Visually observe using plots\n",
    "    * Plot both predicted and true values for a visual comparison\n",
    "* Risidual histograms\n",
    "    * The risidual is the difference between the true target and the predicted target.\n",
    "    * Ideally centered around 0 with a bell shape, which means errors are random in nature, not inherent in the model.\n",
    "* Evaluate with metrics like root mean square error\n",
    "    * Metrics handy for quantifying model performance\n",
    "    * Square the diff between actual and predicted values, and find the average.\n",
    "\n",
    "Look at [this notebook[(https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/PerformanceEvaluation/regression_model_performance.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier Performance\n",
    "\n",
    "Pass/fail, true/false, 1/0\n",
    "\n",
    "* Positive class - the condition we are interested in detecting\n",
    "* Negative class - the normal condition\n",
    "\n",
    "Example: students to be admitted, positive class is admitted, negative are not addmitted\n",
    "Example: individuals at risk of heart diseast, positive class is those are risk, negative are those not at risk\n",
    "\n",
    "Typically use the positive class is the class the algorithm needs to detect.\n",
    "\n",
    "Some algorithms produce a binary output, some provide a raw score that is the probability of being positive. For the latter identify a cut off value, less than is negative.\n",
    "\n",
    "Compare output to label to determine performance.\n",
    "\n",
    "Evaluation techniques, binary outputs:\n",
    "\n",
    "* Plots\n",
    "* [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "* Metrics like recall and precision\n",
    "\n",
    "### Plots\n",
    "\n",
    "* Look at the [binary rawscore performance evaluation notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/PerformanceEvaluation/binary_classifier_rawscore_evaluation.ipynb) for some plot examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "* Useful in evaluating the performance of binary classifiers\n",
    "\n",
    "|                  | Predicted Positive | Predicted Negative |\n",
    "|------------------|--------------------|-----------------|\n",
    "| Actual Positive  | True positive      | False  negative |\n",
    "| Actual Negative  | False positive     | True negative   |\n",
    "\n",
    "True Positive tells us how many samples were correctly classified as positive. True Negative tells us how many samples were correctly classified as negative. False negative tells us how many positive samples were misclassified and false positive tells us how many negative samples were misclassified.\n",
    "\n",
    "If you sum the values in a row, you get the number of true positives and negatives. If you sum the values in a column you get the predicted positives and negatives.\n",
    "\n",
    "Can also use fractional values in the matrix by dividing the positive row by the number of actual positives, and the negative row by the number of actual negatives. This gives true positice rate, false negative rate, etc.\n",
    "\n",
    "Confusion matrix can be computed using sklearn - see [here](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/PerformanceEvaluation/binary_classifier_performance.ipynb)\n",
    "\n",
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review - lecture 29\n",
    "\n",
    "True positive rate\n",
    "\n",
    "* Aka TPR, Recall, Probability of Detection\n",
    "* Count of samples correctly classified as positive / Number of actual positives\n",
    "    * TP/(TP + FN)\n",
    "* Recall value closer to 1 better, closer to 0 worse\n",
    "* Example: radar operator watching skies for enemy planes.\n",
    "    * Positive class is enemy plane, negative is friendly plane\n",
    "    * true positive or probability of detection - the probability of correctly classifying an enemy plane\n",
    "\n",
    "True negative rate\n",
    "\n",
    "* Count of samples correctly classified as negative / Number of actual negatives\n",
    "* Closest to 1 is better, closer to 0 is worse\n",
    "* The probability of correctly classifying a friendly plane\n",
    "\n",
    "False positive rate\n",
    "\n",
    "* FPR, Probability of false alarm\n",
    "* How many negatives were falsely classified as positives (fraction)\n",
    "* Count of negative samples mis-correctly classified as positive / Count of actual negatives\n",
    "* Values closer to 0 are better, closer to 1 are worse\n",
    "* Probability of false alarm - probability of misclassifying a fiendly plane as an enemy plane\n",
    "\n",
    "False negative rate\n",
    "\n",
    "* FNR, Missies\n",
    "* How many positives were misclassified as negative (fraction)\n",
    "* Count of positive samples miscorrectly classified as negative / Count of actual positives\n",
    "* Closer to 0 is better, closer to 1 worse\n",
    "* Probility of misclassifying an enemy plane as a friendly plane\n",
    "\n",
    "Precision\n",
    "\n",
    "* True Positive / (True Positive + False Positive)\n",
    "* How many positives classified by the algoritm are really positive?\n",
    "* Closer to 1 better\n",
    "* Precision would go up as enemy planes are correctly identified, while minimizing false alarms\n",
    "\n",
    "Accuracy\n",
    "\n",
    "* Measure of overall performance\n",
    "* (True positives + True negatives) / (Positive + Negative)\n",
    "* How many positives and negatives were correctly classified (fraction)\n",
    "* Closer to 1 better\n",
    "* Not a good for skewed datasets\n",
    "* Accuracy would go up when enemy planes and friendly planes are correctly identified\n",
    "\n",
    "F1 Score\n",
    "\n",
    "* aka [Harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean)\n",
    "* 2(precision)(recall) / (precision + recall)\n",
    "* closer to 1 better\n",
    "* Another measure of how well the model can identify positives\n",
    "\n",
    "Sklearn has a classification_report function available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under the Curve Metrics\n",
    "\n",
    "Some binary classifiers are based on algoriths that generate a score between 0 and 1, with a custoff threshold used to partition the two classes.\n",
    "\n",
    "* Often 0.5 is used as the cut off\n",
    "* Sometimes a higher threshold makes sense, for example 0.8 before something is classified as spam, reducing the number of regular email classified as spam with the tradeoff of a higher false negative rate.\n",
    "* Sometimes a lower threshold makes sense, for example 0.3 to detect a disease with the tradeoff of a high false alarm rate.\n",
    "\n",
    "Can use area under the curve to evaluate performance under different thresholds - see [this](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/PerformanceEvaluation/binary_classifier_rawscore_evaluation.ipynb) notebook.\n",
    "\n",
    "AUC refers to Area Under Curve. The curve here refers to the plot that has probability of false alarm also known as false positive rate in the x-axis and the probability of detection also known as true positive rate, recall in the y-axis. This curve is called as Receiver Operating Characteristics (ROC).\n",
    "\n",
    "By plotting False Alarm versus Recall at various cutoff thresholds, we can form a curve. A good model has an AUC closer to 1. 0.5 is a random guess, closer to 0 is unusual and indicates the model is flipping results.\n",
    "\n",
    "Can use the sklearn roc_auc_score method from sklearn.metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classifier Performance\n",
    "\n",
    "Multiclass classification algorithms predict one of many classes\n",
    "\n",
    "* Example: grade prediction - A, B, C, D\n",
    "* Classifier assigns a score for each class indicating likelyhood of the sample belonging to that class\n",
    "* Can use the same metrics as per binary classifier, using one versus all\n",
    "* See [this notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/PerformanceEvaluation/multiclass_classifier_performance.ipynb)\n",
    "* See also the [AWS docs](https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-classification.html)\n",
    "\n",
    "Class level vs model level metrics - average class level metrics to get model performance\n",
    "\n",
    "* See [here](https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin)\n",
    "* Macro - doesn't consider number of samples of each class, not a good measure for skewed data\n",
    "* Weighted - assigns weight to each class based on samples if each class, better as it accounts for skewed distributions\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
