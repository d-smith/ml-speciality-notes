{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance\n",
    "\n",
    "Terminology\n",
    "\n",
    "* Labeled data - split into *training set* and *test set*\n",
    "\n",
    "Model fit - see [here](https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html)\n",
    "\n",
    "* Underfitting - performs poorly on training and test set. \n",
    "    * Model fails to capture relationship between inputs and output.\n",
    "    * Fix by adding features, adding more complex features, adding more examples to the training set, optimize hyper parameters\n",
    "* Overfitting - performs well on the training data, and poorly on the test data\n",
    "    * Model 'memorizes' the training data, and fails to generalize to unseen data\n",
    "    * Correct by removing more complex features,optimize hyper parameters\n",
    "    \n",
    "Supervised Learning Algorithm Types\n",
    "\n",
    "* Regression - output is continuous numeric\n",
    "* Binary classification - output is binary\n",
    "* Multi-class classification - Categorical - one of many possible outcomes\n",
    "\n",
    "## Regression Model Performance\n",
    "\n",
    "Common techniques for evaluating model performance:\n",
    "\n",
    "* Visually observe using plots\n",
    "    * Plot both predicted and true values for a visual comparison\n",
    "* Risidual histograms\n",
    "    * The risidual is the difference between the true target and the predicted target.\n",
    "    * Ideally centered around 0 with a bell shape, which means errors are random in nature, not inherent in the model.\n",
    "* Evaluate with metrics like root mean square error\n",
    "    * Metrics handy for quantifying model performance\n",
    "    * Square the diff between actual and predicted values, and find the average.\n",
    "\n",
    "Look at [this notebook[(https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/PerformanceEvaluation/regression_model_performance.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier Performance\n",
    "\n",
    "Pass/fail, true/false, 1/0\n",
    "\n",
    "* Positive class - the condition we are interested in detecting\n",
    "* Negative class - the normal condition\n",
    "\n",
    "Example: students to be admitted, positive class is admitted, negative are not addmitted\n",
    "Example: individuals at risk of heart diseast, positive class is those are risk, negative are those not at risk\n",
    "\n",
    "Typically use the positive class is the class the algorithm needs to detect.\n",
    "\n",
    "Some algorithms produce a binary output, some provide a raw score that is the probability of being positive. For the latter identify a cut off value, less than is negative.\n",
    "\n",
    "Compare output to label to determine performance.\n",
    "\n",
    "Evaluation techniques, binary outputs:\n",
    "\n",
    "* Plots\n",
    "* Confusion matrix\n",
    "* Metrics like recall and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
