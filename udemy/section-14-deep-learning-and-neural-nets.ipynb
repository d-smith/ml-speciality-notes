{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Deep learning - Linear Regression\n",
    "\n",
    "Set of input features and weights used to calculated a real number output\n",
    "\n",
    "* $ x_0 $ is the intercept with an initial value of 1\n",
    "* $ y = w_0 x_0 + w_1 x_1 + ... + w_m x_m = \\sum_{i=0}^N w_i X_i$\n",
    "\n",
    "Developing the model is the matter of assigning the weights to the features.\n",
    "\n",
    "* [Example notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/GradientDescent/linear_cost_example.ipynb)\n",
    "\n",
    "Notes based on the notebook.\n",
    "\n",
    "* Simple data set - one feature, target is a line plus noise\n",
    "* Create a data set with some random noise - straight line with noice\n",
    "* Fit with linear regression\n",
    "\n",
    "How did the algorithm determine the weights?\n",
    "\n",
    "* Need a way to measure how close a predicton is to ground truth\n",
    "    * Squared loss error function (aka mean squared error)\n",
    "    * Loss function measures how close the predicted value is to ground truth\n",
    "    \n",
    "* Gradient descent optimizer used to determine the weights\n",
    "    * Plot the loss at different weight - plot is parabolic \n",
    "    * Algorithm starts with random wights\n",
    "    * Gradient (slope) of curve lets us know which way to go (larger or smaller) to increase or decrease the loss\n",
    "    * Negative slope - increasing weight moves downhill\n",
    "    * Positive slope - decreasing weight moves downhill\n",
    "    \n",
    "* Magnitude of weight adjustments\n",
    "    * Learning rate determines the size of the weight adjustment, tradeoff is number of iterations vs ability to converge on the optimal weight\n",
    "    * [More info on optimizing gradient descent](https://ruder.io/optimizing-gradient-descent/)\n",
    "    * Some adjust learning rate based on degree of slope, some use momentum, etc.\n",
    "    \n",
    "Gradient descent modes\n",
    "\n",
    "* Batch\n",
    "    * Compute loss for all training examples\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set. For each iteration, weight is adjusted once\n",
    "* Stochastic\n",
    "    * Compute loss for next example\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set. For each iteration, weight is adjusted 150 times\n",
    "* Mini-batch\n",
    "    * Compute loss for a specified number of examples\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set, mini batch size is 15. For each iteration, weight is adjusted 10 times.\n",
    "\n",
    "## Logistic Regression (Binary Classification)\n",
    "\n",
    "Set up is similar to linear regression - we have a set of features and assign a weight to each feature, we sum the products of features and weights. But, for output we want to know probability of the output belonging to the positive class, based on an output of 0 or 1.\n",
    "\n",
    "We can use the sigmoid function to run the output of the sum through - sigmoid function output is bounded between zero and one - see [this](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/GradientDescent/logistic_cost_example.ipynb) notebook.\n",
    "\n",
    "* Typically need to assign a cut off value for the output, for example anything greater than 0.5 is positive.\n",
    "\n",
    "Training objective with logistic regression is to select the weights that lowers the misclassification.\n",
    "\n",
    "* Use the logistic cost loss function, which separates negative and positive values (loss curves for positive and negative samples)\n",
    "* Logistic loss function is parabolic in nature, also with the property of not only indicating the loss at a given weight, but also indicating which direction to adjust the weight.\n",
    "\n",
    "How to find the optimal weights?\n",
    "\n",
    "* Use the gradient descent optimizer\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "Linear models are simple and easy to understand, but typically underperform on non linear data (underfit). They require extensive feature engineering, features need to be on similar range and scale.\n",
    "\n",
    "Linear models form the foundation for understanding neural networks. NN looks like stacking several logistic models, generalizing sigmoid with an activation function.\n",
    "\n",
    "Summation of features plus weights ran through an activation function is a 'neuron', at each layer the features can be connected to multiple neurons with the weights specific to each neuron.\n",
    "\n",
    "* The neurons generate new features by combining existing ones, which are then inputs to the next layer of neurons.\n",
    "* Basic architecture has an input layer, one or more hidden layers, and an output layer.\n",
    "\n",
    "Benefits\n",
    "\n",
    "* Automatic feature engineering - mixes features to create new ones\n",
    "* Handles non-linear datasets\n",
    "* Standard techniques to deal with overfitting  (easy to overfit) - regularization, reduce model complexity, etc.\n",
    "\n",
    "Activation Functions\n",
    "\n",
    "* Introduce non-linearity into the model\n",
    "* Improves ability of model to fit complex non-linear datasets\n",
    "* Three popular activation fucntions: sigmoid, tanh, relu\n",
    "\n",
    "Activation function notebook - see [here](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/GradientDescent/activation_functions.ipynb)\n",
    "\n",
    "* sigmoid - converts input to a number between 0 and 1\n",
    "* tanh - output varies from -1 to 1\n",
    "* relu - netgative input output is 0, otherwise same as input\n",
    "\n",
    "Deep learning - subset of machine learning that uses complex networks that have hundreds of layers. Why so popular?\n",
    "\n",
    "* Traditional ML algorithms appear to saturate on how much they can learn. Having massive amounts of data does not translate to more learning\n",
    "* Small NN can learn better. Medium NN can learn even more, and large NNs can keep learning with more data.\n",
    "\n",
    "Binary classifier - send the output through a sigmoid function.\n",
    "\n",
    "Multiclass classifier - use softmax to convert to array of probability scores for each class, sum of probs for all classes is 1.\n",
    "\n",
    "Popular NN architectures\n",
    "\n",
    "* General purpose\n",
    "    * fully connected network\n",
    "    * example: treats each pixel as a separate feature\n",
    "* Convolutional Neural Network (CNN)\n",
    "    * Useful for image analysis\n",
    "    * Example: considers pixels and its surrounding pixels\n",
    "* Recurrent NN\n",
    "    * Looks at history\n",
    "    * Used for timeseries prediction, natural language processing\n",
    "    * Example: timeseries forcasting - model looks at current values and historical values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
