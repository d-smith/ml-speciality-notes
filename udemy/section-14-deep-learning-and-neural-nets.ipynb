{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Deep learning - Linear Regression\n",
    "\n",
    "Set of input features and weights used to calculated a real number output\n",
    "\n",
    "* $ x_0 $ is the intercept with an initial value of 1\n",
    "* $ y = w_0 x_0 + w_1 x_1 + ... + w_m x_m = \\sum_{i=0}^N w_i X_i$\n",
    "\n",
    "Developing the model is the matter of assigning the weights to the features.\n",
    "\n",
    "* [Example notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/GradientDescent/linear_cost_example.ipynb)\n",
    "\n",
    "Notes based on the notebook.\n",
    "\n",
    "* Simple data set - one feature, target is a line plus noise\n",
    "* Create a data set with some random noise - straight line with noice\n",
    "* Fit with linear regression\n",
    "\n",
    "How did the algorithm determine the weights?\n",
    "\n",
    "* Need a way to measure how close a predicton is to ground truth\n",
    "    * Squared loss error function (aka mean squared error)\n",
    "    * Loss function measures how close the predicted value is to ground truth\n",
    "    \n",
    "* Gradient descent optimizer used to determine the weights\n",
    "    * Plot the loss at different weight - plot is parabolic \n",
    "    * Algorithm starts with random wights\n",
    "    * Gradient (slope) of curve lets us know which way to go (larger or smaller) to increase or decrease the loss\n",
    "    * Negative slope - increasing weight moves downhill\n",
    "    * Positive slope - decreasing weight moves downhill\n",
    "    \n",
    "* Magnitude of weight adjustments\n",
    "    * Learning rate determines the size of the weight adjustment, tradeoff is number of iterations vs ability to converge on the optimal weight\n",
    "    * [More info on optimizing gradient descent](https://ruder.io/optimizing-gradient-descent/)\n",
    "    * Some adjust learning rate based on degree of slope, some use momentum, etc.\n",
    "    \n",
    "Gradient descent modes\n",
    "\n",
    "* Batch\n",
    "    * Compute loss for all training examples\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set. For each iteration, weight is adjusted once\n",
    "* Stochastic\n",
    "    * Compute loss for next example\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set. For each iteration, weight is adjusted 150 times\n",
    "* Mini-batch\n",
    "    * Compute loss for a specified number of examples\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set, mini batch size is 15. For each iteration, weight is adjusted 10 times.\n",
    "\n",
    "## Logistic Regression (Binary Classification)\n",
    "\n",
    "Set up is similar to linear regression - we have a set of features and assign a weight to each feature, we sum the products of features and weights. But, for output we want to know probability of the output belonging to the positive class, based on an output of 0 or 1.\n",
    "\n",
    "We can use the sigmoid function to run the output of the sum through - sigmoid function output is bounded between zero and one - see [this](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/GradientDescent/logistic_cost_example.ipynb) notebook.\n",
    "\n",
    "* Typically need to assign a cut off value for the output, for example anything greater than 0.5 is positive.\n",
    "\n",
    "Training objective with logistic regression is to select the weights that lowers the misclassification.\n",
    "\n",
    "* Use the logistic cost loss function, which separates negative and positive values (loss curves for positive and negative samples)\n",
    "* Logistic loss function is parabolic in nature, also with the property of not only indicating the loss at a given weight, but also indicating which direction to adjust the weight.\n",
    "\n",
    "How to find the optimal weights?\n",
    "\n",
    "* Use the gradient descent optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
