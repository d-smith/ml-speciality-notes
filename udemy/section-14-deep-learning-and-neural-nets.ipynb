{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Deep learning - Linear Regression\n",
    "\n",
    "Set of input features and weights used to calculated a real number output\n",
    "\n",
    "* $ x_0 $ is the intercept with an initial value of 1\n",
    "* $ y = w_0 x_0 + w_1 x_1 + ... + w_m x_m = \\sum_{i=0}^N w_i X_i$\n",
    "\n",
    "Developing the model is the matter of assigning the weights to the features.\n",
    "\n",
    "* [Example notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/GradientDescent/linear_cost_example.ipynb)\n",
    "\n",
    "Notes based on the notebook.\n",
    "\n",
    "* Simple data set - one feature, target is a line plus noise\n",
    "* Create a data set with some random noise - straight line with noice\n",
    "* Fit with linear regression\n",
    "\n",
    "How did the algorithm determine the weights?\n",
    "\n",
    "* Need a way to measure how close a predicton is to ground truth\n",
    "    * Squared loss error function (aka mean squared error)\n",
    "    * Loss function measures how close the predicted value is to ground truth\n",
    "    \n",
    "* Gradient descent optimizer used to determine the weights\n",
    "    * Plot the loss at different weight - plot is parabolic \n",
    "    * Algorithm starts with random wights\n",
    "    * Gradient (slope) of curve lets us know which way to go (larger or smaller) to increase or decrease the loss\n",
    "    * Negative slope - increasing weight moves downhill\n",
    "    * Positive slope - decreasing weight moves downhill\n",
    "    \n",
    "* Magnitude of weight adjustments\n",
    "    * Learning rate determines the size of the weight adjustment, tradeoff is number of iterations vs ability to converge on the optimal weight\n",
    "    * [More info on optimizing gradient descent](https://ruder.io/optimizing-gradient-descent/)\n",
    "    * Some adjust learning rate based on degree of slope, some use momentum, etc.\n",
    "    \n",
    "Gradient descent modes\n",
    "\n",
    "* Batch\n",
    "    * Compute loss for all training examples\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set. For each iteration, weight is adjusted once\n",
    "* Stochastic\n",
    "    * Compute loss for next example\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set. For each iteration, weight is adjusted 150 times\n",
    "* Mini-batch\n",
    "    * Compute loss for a specified number of examples\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set, mini batch size is 15. For each iteration, weight is adjusted 10 times.\n",
    "\n",
    "## Logistic Regression (Binary Classification)\n",
    "\n",
    "Set up is similar to linear regression - we have a set of features and assign a weight to each feature, we sum the products of features and weights. But, for output we want to know probability of the output belonging to the positive class, based on an output of 0 or 1.\n",
    "\n",
    "We can use the sigmoid function to run the output of the sum through - sigmoid function output is bounded between zero and one - see [this](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/GradientDescent/logistic_cost_example.ipynb) notebook.\n",
    "\n",
    "* Typically need to assign a cut off value for the output, for example anything greater than 0.5 is positive.\n",
    "\n",
    "Training objective with logistic regression is to select the weights that lowers the misclassification.\n",
    "\n",
    "* Use the logistic cost loss function, which separates negative and positive values (loss curves for positive and negative samples)\n",
    "* Logistic loss function is parabolic in nature, also with the property of not only indicating the loss at a given weight, but also indicating which direction to adjust the weight.\n",
    "\n",
    "How to find the optimal weights?\n",
    "\n",
    "* Use the gradient descent optimizer\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "Linear models are simple and easy to understand, but typically underperform on non linear data (underfit). They require extensive feature engineering, features need to be on similar range and scale.\n",
    "\n",
    "Linear models form the foundation for understanding neural networks. NN looks like stacking several logistic models, generalizing sigmoid with an activation function.\n",
    "\n",
    "Summation of features plus weights ran through an activation function is a 'neuron', at each layer the features can be connected to multiple neurons with the weights specific to each neuron.\n",
    "\n",
    "* The neurons generate new features by combining existing ones, which are then inputs to the next layer of neurons.\n",
    "* Basic architecture has an input layer, one or more hidden layers, and an output layer.\n",
    "\n",
    "Benefits\n",
    "\n",
    "* Automatic feature engineering - mixes features to create new ones\n",
    "* Handles non-linear datasets\n",
    "* Standard techniques to deal with overfitting  (easy to overfit) - regularization, reduce model complexity, etc.\n",
    "\n",
    "Activation Functions\n",
    "\n",
    "* Introduce non-linearity into the model\n",
    "* Improves ability of model to fit complex non-linear datasets\n",
    "* Three popular activation fucntions: sigmoid, tanh, relu\n",
    "\n",
    "Activation function notebook - see [here](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/GradientDescent/activation_functions.ipynb)\n",
    "\n",
    "* sigmoid - converts input to a number between 0 and 1\n",
    "* tanh - output varies from -1 to 1\n",
    "* relu - netgative input output is 0, otherwise same as input\n",
    "\n",
    "Deep learning - subset of machine learning that uses complex networks that have hundreds of layers. Why so popular?\n",
    "\n",
    "* Traditional ML algorithms appear to saturate on how much they can learn. Having massive amounts of data does not translate to more learning\n",
    "* Small NN can learn better. Medium NN can learn even more, and large NNs can keep learning with more data.\n",
    "\n",
    "Binary classifier - send the output through a sigmoid function.\n",
    "\n",
    "Multiclass classifier - use softmax to convert to array of probability scores for each class, sum of probs for all classes is 1.\n",
    "\n",
    "Popular NN architectures\n",
    "\n",
    "* General purpose\n",
    "    * fully connected network\n",
    "    * example: treats each pixel as a separate feature\n",
    "* Convolutional Neural Network (CNN)\n",
    "    * Useful for image analysis\n",
    "    * Example: considers pixels and its surrounding pixels\n",
    "* Recurrent NN\n",
    "    * Looks at history\n",
    "    * Used for timeseries prediction, natural language processing\n",
    "    * Example: timeseries forcasting - model looks at current values and historical values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIT Introduction to Deep Learning - Lecture 1\n",
    "\n",
    "Slides [here](http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L1.pdf)\n",
    "Video [here](https://www.youtube.com/watch?v=5v1JnYv_yWs&feature=youtu.be)\n",
    "\n",
    "### Why deep learning?\n",
    "\n",
    "* Traditional ML algorithms - hand engineer features, which is time consuming, brittle, not scalable\n",
    "* Can we learn the features directly from raw data?\n",
    "    * lines and edges to eyes and noses to faces\n",
    "* Why now?\n",
    "    * big data (more and larger data sets, easier collection and storage), hardware (GPUs, parallel processing), software (improved techniques, new models, open source)\n",
    "    \n",
    "### The Perceptron - the structural block of deep learning\n",
    "\n",
    "Feed-forward: inputs, weights, sum, non-linearity, output\n",
    "\n",
    "$ \\hat y = g(\\sum_{i=1}^m x_i w_i)$ where $ g $ is the activation function\n",
    "\n",
    "Note we also have another term, the bias term, which lets us shift the activation left or right:\n",
    "\n",
    "$ \\hat y = g(w_0 + \\sum_{i=1}^m x_i w_i)$ where $ w_0 $ is the bias term.\n",
    "\n",
    "We can rewrite this linear algebra style:\n",
    "\n",
    "$ \\hat y = g( w_0 + X^TW) $ where:\n",
    "\n",
    "$ X = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_m \\end{bmatrix} $ and $ W = \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_m \\end{bmatrix} $\n",
    "\n",
    "\n",
    "Activation function - typically a non-linear function like the sigmoid function, e.g.\n",
    "\n",
    "$ \\sigma (z) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-z} }  $\n",
    "\n",
    "Can also be tanh or ReLU\n",
    "\n",
    "\n",
    "### Importance of Activation Functions\n",
    "\n",
    "The purpose is to introduce non-linearities into the network, as many problems in the real world involve non-linearities.\n",
    "    * Think about nonlinear decision boundaries\n",
    "    \n",
    "    \n",
    "### Building Neural Networks with Perceptrons\n",
    "\n",
    "3 steps to computing the output of a perceptron: dot product, add a bias, take a non-linearity\n",
    "\n",
    "$ y = g(z) $ where $ z = w_0 + \\sum_{j=1}^m x_j w_j $\n",
    "\n",
    "Multi Output Perceptron\n",
    "\n",
    "$ y_1 = g(z_1) $\n",
    "$ y_2 = g(z_2) $\n",
    "$ z_i = w_0,_i + \\sum_{j=1}^m x_j w_j,_i $\n",
    "\n",
    "Single Layer Nueral Network\n",
    "\n",
    "* Input layer, fully connected hidden layer, output layer (two outputs)\n",
    "\n",
    "$ z_i = w^{(1)}_{0,i} + \\sum_{j=1}^m x_j w^{(1)}_{j,i} $\n",
    "\n",
    "and...\n",
    "\n",
    "$ \\hat {y_i} = g(w^{(2)}_{0,i} + \\sum_{j=1}^{d_1} z_j w^{(2)}_{0,i})$\n",
    "\n",
    "Middle layer has $ z_1 ... z_{d_1} $ nodes\n",
    "\n",
    "The hidden layer is learning, not observable like the input layer and the output layer.\n",
    "\n",
    "Another name for fully connected layers is a dense layer.\n",
    "\n",
    "In Keras/TF:\n",
    "\n",
    "```\n",
    "from tf.keras.layers import *\n",
    "\n",
    "inputs = Inputs(m)\n",
    "hidden = Dense(d1)(inputs)\n",
    "outputs = Dense(2)(hidden)\n",
    "model = Model(inputs, outputs)\n",
    "```\n",
    "\n",
    "\n",
    "### Applying Neural Networks\n",
    "\n",
    "Will I pass this class?\n",
    "\n",
    "* Two inputs: hours spend on the final project, number of lectures attended\n",
    "* Output: pass/fail\n",
    "\n",
    "How to train the model? First need to know how to quantify the loss.\n",
    "\n",
    "$ L(f(x^{(i)};W), y^{(i)}) $ (compares predicted and actual value)\n",
    "\n",
    "Loss is low if close to actual, higher if not.\n",
    "\n",
    "Empiracal loss - measures the total over out entire dataset.\n",
    "\n",
    "* aka Objective function, cost function, empirical risk\n",
    "\n",
    "$ J(W) = \\frac{1}{n} \\sum_{i=1}^n L(f(x^{(i)};W), y^{(i)})$\n",
    "\n",
    "0/1 Output - use Binary Cross Entropy Loss\n",
    "Computer a grade or number output - use Mean Squared Error Loss\n",
    "\n",
    "### Training Neural Networks\n",
    "\n",
    "Training is about loss optimization. We want to find the network weights that achive the lowest loss\n",
    "\n",
    "$ W^{*} = \\frac{argmin}{W} \\frac{1}{n} \\sum_{i=1}^n L(f(x^{(i)};W), y^{(i)}) $\n",
    "\n",
    "$ W^{*} = \\frac{argmin}{W} J(W) $\n",
    "\n",
    "Remember: $ W = \\{ W^{(0)},W^{(1)},... \\} $\n",
    "\n",
    "We find the optimal weights via Gradient Descent\n",
    "\n",
    "Algoritm:\n",
    "\n",
    "1. Initiaize weights randomly $ \\sim N(O, \\sigma^2 )$\n",
    "2. Loop until convergence\n",
    "\n",
    "    1. Compute gradient $ \\frac{\\partial{J(W)}}{\\partial{W}} $\n",
    "    2. Update weights $ W \\leftarrow W - \\eta \\frac{\\partial{J(W)}}{\\partial{W}} $\n",
    "5. Return weights\n",
    "\n",
    "```\n",
    "weights = tf.random_nomal(shape, stddev=sigma)\n",
    "grads = tf.gradients(ys=loss,xs=weights)\n",
    "weights_new=weights.assign(weights = lr * grads)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the Gradients: Backpropagation\n",
    "\n",
    "Single network: $x \\rightarrow w_1 \\rightarrow z_1 \\rightarrow w_2 \\rightarrow \\hat y \\rightarrow J(W)$\n",
    "\n",
    "How does a small change in weight ($w_2$) affect the final loss $ J(W) $?\n",
    "\n",
    "Unpack using the chain rule\n",
    "\n",
    "$ \\frac{\\partial{J(W)}}{\\partial{w_2}} = \\frac{\\partial{J(W)}}{\\partial{\\hat y}} * \\frac{\\partial{\\hat y}}{\\partial{w_2}}$\n",
    "\n",
    "And the influence of w1? Apply the chain rule again\n",
    "\n",
    "$ \\frac{\\partial{J(W)}}{\\partial{w_1}} = \\frac{\\partial{J(W)}}{\\partial{\\hat y}} * \\frac{\\partial{\\hat y}}{\\partial{z_1}} * \\frac{\\partial{z_1}}{\\partial{w_1}}$\n",
    "\n",
    "Repeat this for every weight in the enetwork using gradients from later layers.\n",
    "\n",
    "\n",
    "### Neural Networks in Practice: Optimization\n",
    "\n",
    "In practice training neural networks is difficult. Loss landscape is complex with many local optima. Loss optimization can be difficult to optimize.\n",
    "\n",
    "Setting the learning rate $ \\eta $ is difficult and can greatly affect the optimization.\n",
    "\n",
    "How to deal with this?\n",
    "\n",
    "* Try a lot of learning rates to see what works best.\n",
    "* Adaptive learning rates - don't fix the rate, but adjustis based on how large the gradient is, how fast learning is happenig, zied of particular weights, etc.\n",
    "    * Tensor flow examples: momentum, adagrad, adadelta, adam, rmsprop\n",
    "    \n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "Just compute at a single point to save some cycles... but noisy\n",
    "\n",
    "1. Initiaize weights randomly $ \\sim N(O, \\sigma^2 )$\n",
    "2. Loop until convergence\n",
    "    1. Pick single data point $i$\n",
    "    1. Compute gradient $ \\frac{\\partial{J_i(W)}}{\\partial{W}} $\n",
    "    2. Update weights $ W \\leftarrow W - \\eta \\frac{\\partial{J(W)}}{\\partial{W}} $\n",
    "5. Return weights\n",
    "\n",
    "### Mini-batches\n",
    "\n",
    "Easier to compute, less noise than stochastic as you are considering a wider population\n",
    "\n",
    "1. Initiaize weights randomly $ \\sim N(O, \\sigma^2 )$\n",
    "2. Loop until convergence\n",
    "    1. Pick batch of B data points\n",
    "    1. Compute gradient $ \\frac{\\partial{J_i(W)}}{\\partial{W}} = \\frac{1}{B}\\sum_{k=1}^B \\frac{\\partial{J_k(W)}}{\\partial{W}} $\n",
    "    2. Update weights $ W \\leftarrow W - \\eta \\frac{\\partial{J(W)}}{\\partial{W}} $\n",
    "5. Return weights\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Want a model that performs well and generalizes well\n",
    "\n",
    "* Underfit - model not complex enough to fully learn\n",
    "* Ideal fit\n",
    "* Overfitting - too complex, extra parameters, memorizes the training set, does not generalize well\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "Regularization is a technique that constrains our optimization problem to discourage complex models. Why do we need it? To improve generalization of our model to unseen model.\n",
    "\n",
    "Regularization 1: Drop out\n",
    "\n",
    "* During training, randomly set some activations to 0\n",
    "    * Typically drop 50% of activations in a layer in any given training iteration\n",
    "    * Creates an ensemble of multiple models through the paths\n",
    "    * Forces network to not rely on any 1 node\n",
    "    \n",
    "Regularization 2: Early stopping\n",
    "\n",
    "* Stop training before we have a change to overfit.\n",
    "    * Usually prior to where the loss of the testing performance starts rising\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIT 6.S191 (2019): Convolutional Neural Networks\n",
    "\n",
    "Lecture 3 from Introduction to Deep Learning\n",
    "\n",
    "Video [here](https://www.youtube.com/watch?v=H-HVZJ7kGI0&feature=youtu.be)\n",
    "Slides [here](http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L3.pdf)\n",
    "\n",
    "Image recognition\n",
    "\n",
    "* Input is a 2D image, vector of pixel values\n",
    "* Output - class label, cna produce probability of belonging to a particular class\n",
    "\n",
    "Fully connected network architecture\n",
    "\n",
    "* Not a good fit for vision processing\n",
    "* Squashing a 2D matrix into a vector and fully connecting it means you lost spatial information, and connectivity makes computation too expensive\n",
    "\n",
    "Goal: use an architecture that uses the spatial structure\n",
    "\n",
    "* connect patches of input to neurons in hidden layer\n",
    "* Pixels next to each other are probably realted\n",
    "* Use a sliding patch windows across the image to define connections\n",
    "\n",
    "How to weight to weight the patch to detect particular features?\n",
    "\n",
    "* Apply a set of weights - a filter - to extract local features.\n",
    "* Use multiple filters to extract different features\n",
    "* Spatially share parameters of each filter\n",
    "\n",
    "Feature extraction with convolution\n",
    "\n",
    "* Filter of size 4x4: 16 different weights\n",
    "* Apply this same filter to 4x4 patches in the input\n",
    "* Shift by 2 pixels for next patch\n",
    "\n",
    "Convolution\n",
    "\n",
    "* Uses filters to identify where features expressed in filter 'pop up' in the image\n",
    "* Perform element wise multiplication of patch with location of window on image\n",
    "\n",
    "Example - 5x5 image convolved with a 3x3 filter\n",
    "\n",
    "* Yields 3x3 feature map\n",
    "\n",
    "Feature extraction with convolution: \n",
    "\n",
    "* Different fiters can extract different features based on the spatial structure inherent in the data.\n",
    "    * Apply a set of wieghts - a filter - to extract local features\n",
    "    * Use multiple filters to extract different features\n",
    "    * Spatially share parameters of each filter\n",
    "    \n",
    "Convolutional Neural Networks - CNNs\n",
    "\n",
    "1. Convolution: apply filters with learned weights to generate feature maps\n",
    "2. Apply non-linearity: often RELU\n",
    "3. Pooling: downsampling operation on each feature map\n",
    "    * reduce diminsionality\n",
    "    * retain spatial invariance\n",
    "\n",
    "Train model with image data, learn weights of filters in convolutional layers.\n",
    "\n",
    "* What features in the image are we learning\n",
    "\n",
    "Image input -> convolution (feature maps) -> maxpooling -> fully connected output layer\n",
    "\n",
    "ImageNet\n",
    "\n",
    "* Most famous data set for training CNNs\n",
    "* 14 million images across 21,841 \n",
    "\n",
    "Architecture for Many Applications\n",
    "\n",
    "* Feature learning part, classification part\n",
    "* Uses\n",
    "    * Semantic segmentation\n",
    "    * Object detection\n",
    "    * Image captioning\n",
    "    \n",
    "Semantic Segmentation\n",
    "\n",
    "* Task is to assign each pixel of the image to an object\n",
    "* FCN: fully convolutional networks\n",
    "    * Network design with all convolutional laters\n",
    "    * Input and output sizes the same\n",
    "    * Convolutional layers with downsampling and upsampling operations\n",
    "* Applied to real time driving scenes for example\n",
    "\n",
    "Object Detection\n",
    "\n",
    "* R-CNN: \n",
    "    * Find regions we think have objects \n",
    "    * Use CNN to classify.\n",
    "    \n",
    "Image Captioning\n",
    "\n",
    "* Generate a sentance that describes the scene\n",
    "* Classify images with a CNN\n",
    "* Connect to an RNN to generate the sentance\n",
    "* Fixed output of CNN used to initiate the RNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIT 6.S191: Recurrent Neural Networks\n",
    "\n",
    "Video [here](https://www.youtube.com/watch?v=_h66BW-xNgk&feature=youtu.be)\n",
    "Slides [here](http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L2.pdf)\n",
    "\n",
    "Think of the movement of a ball and predicting where it move next?\n",
    "\n",
    "* Much easier to predict where it will go next in space if we know the history of its previous positions\n",
    "\n",
    "Sequences in the wild\n",
    "\n",
    "* Audio - sequence of Sound waves\n",
    "* Text - sequences of words\n",
    "* Stock prices\n",
    "* Genomic data\n",
    "\n",
    "A sequence modeling problem: predict the next word\n",
    "\n",
    "* If we have \"This morning I took my cat for a \" - can we predict the next word?\n",
    "* Need a way to handle variable length input. In a traditional feed forward network we could...\n",
    "    * Use a fixed window, for examples previous two words. Problem - you may need more history than the window holds.\n",
    "    * Bag of words - vector representing words, with count of words in each slot. Problem: does not preserve order.\n",
    "    * Use a really big fixed window. Problem: no parameter sharing, things we learn about the sequence in one point doesn't transfer to another point.\n",
    "    \n",
    "Sequence Modeling - Design Criteria\n",
    "\n",
    "To model sequences, we need to...'\n",
    "\n",
    "* Handle variable-length sequences\n",
    "* Track long-term dependencies\n",
    "* Maintain information about the sequence order\n",
    "* Share parameters learned across the entire sequence\n",
    "\n",
    "RNNs\n",
    "\n",
    "* Standard feed forward architecture - propagate from input to output, in one direction\n",
    "* RNN - sequence of data fed through the model, can return a single output or an output at different points in time (e.g. music generation)\n",
    "* RNNs i input vector, output vector, maintain internal states and passes the internal state from current step to the next step\n",
    "\n",
    "Apply a recurrence relation at every time step to process a squence.\n",
    "\n",
    "$ h_t = f_W(h_{t-1},x_t)$\n",
    "\n",
    "Function is parameterized by a set of weights. Note the same function and set of parameters are used at each step. Each step includes both an output and a state update.\n",
    "\n",
    "Update hidden state:\n",
    "\n",
    "$ h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t)$\n",
    "\n",
    "Output at a timestep is a transformed version of the internal state:\n",
    "\n",
    "$ \\hat y_t = W_{hy}h_t$\n",
    "\n",
    "Can think of an RNN computational graph across time.\n",
    "\n",
    "* Conceptually copies of the network passing state as an input to the next copy in the chain.\n",
    "* Can make the parameters (weight matrices) explicit\n",
    "    * Weights to transform inputs to the hidden state\n",
    "    * Weights to transform the previous hidden state to the next hidden state\n",
    "    * Weights to transform the hidden state to the output\n",
    "* Same weights used throughout\n",
    "\n",
    "Loss function\n",
    "\n",
    "* Computer a loss at each step in the network\n",
    "* Total loss is the sum of all the individual step losses\n",
    "\n",
    "Backpropagation Through Time (BPTT)\n",
    "\n",
    "* Forward pass - forward across time, outputs, states, losses\n",
    "* Backprop - errors are back propagated at each individual time step, and across time steps from where you are to all the way back to the beginning of the squence.\n",
    "* Computing the gradient wrt to $h_0$ involved many factors of $W_{hh}$ (and repeated $f^{'}$)\n",
    "    * Many values > 1: exploding gradients (become very large, difficult to optimize)\n",
    "        * Handle this using gradient clipping to scale large gradients\n",
    "    * Many values < 1: vanishing gradients\n",
    "        * Change the activation function\n",
    "        * Weight initialization\n",
    "        * Network architecture\n",
    "        \n",
    "Why are vanishing gradients a problem?\n",
    "    * Multiply many small numbers together\n",
    "    * Errors due to further back time steps have smaller and smaller gradients\n",
    "    * Bias network to capture short-term dependencies\n",
    "    \n",
    "Activation functions\n",
    "   \n",
    "* Derivitives of tanh and sigmoid usually less than one\n",
    "* ReLU derivitive is either 1 (x > 0) or 0 \n",
    "\n",
    "Initialization\n",
    "\n",
    "* Initializing wieghts to the identity matrix helps preven vanishing\n",
    "\n",
    "Architecture\n",
    "\n",
    "* Use a more complex recurrent unit with gates to control what information is passed through\n",
    "* LSTM, GRU, etc\n",
    "\n",
    "\n",
    "Long Short Term Memory\n",
    "\n",
    "* Standard model has a single computation (e.g. tanh), LSTM has several computing units\n",
    "* LSTMs maintain a cell state where it's easy for information to flow, this is in addition to unit/step state\n",
    "* Single computation for cell state\n",
    "* Inforamtion is added or removed to a cell state through structures called gates\n",
    "    * Optionally let information through via a sigmoid nn layer and pointwise multiplication\n",
    "    \n",
    "How do LSTs work?\n",
    "\n",
    "* *Forget* irrelevant parts of the previous state (forget gate)\n",
    "* Selectively *update* cell state values\n",
    "    * Sigmoid layer to decide what values to update\n",
    "    * tanh layer: generate new vector of candiate value that could be added to the state\n",
    "* Use an output gate to *output* certain parts of the cell state\n",
    "    * Apply forget operation to previous cell state\n",
    "    * Add new candidate values, scaled by how much we decided to update\n",
    "\n",
    "LSTM Output\n",
    "\n",
    "* Output filtered version of cell state\n",
    "\n",
    "LSTM Gradient Flow\n",
    "\n",
    "* Back propagation requires only element-wise multiplication. No matrix multiplication, avoiding the vanishing gradient problem.\n",
    "\n",
    "RNN Applications\n",
    "\n",
    "* Example: music generation\n",
    "    * Input: sheet music\n",
    "    * Output: next character in sheet music\n",
    "* Example: sentiment classification\n",
    "    * Input: sequence of words\n",
    "    * Output: sentiment\n",
    "* Example: machine translation\n",
    "    * Input: sentence in one language\n",
    "    * Output: sentance in a different language\n",
    "    * Attention mechanisms: bring early encoder output to the decoder stages\n",
    "        * AMs in neural networks provide learnable memory access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIPS 2016 tutorial: Nuts and bolts of building AI applications\n",
    "\n",
    "Video [here](https://www.youtube.com/watch?v=wjqaz6m42wU&feature=youtu.be)\n",
    "Slides [here](https://media.nips.cc/Conferences/2016/Slides/6203-Slides.pdf)\n",
    "\n",
    "### Major DL Trends\n",
    "\n",
    "#### Trend: Scale\n",
    "\n",
    "Many old ideas, why are they taking off now?\n",
    "\n",
    "* Older algs performance tapers off, even though there's more data available - learning capacity is limited.\n",
    "* Small NNs had slightly better performance\n",
    "* Mediun even better\n",
    "* Large nets get better and better\n",
    "\n",
    "Scale is driving improvement\n",
    "\n",
    "* Amount of data \n",
    "* Size of networks\n",
    "\n",
    "Note in the small data regime the above ordering of the algoritms in performance may vary.\n",
    "\n",
    "Team Org\n",
    "\n",
    "* ML team has both deep learning specialists, computer systens teams\n",
    "* Leading edge is trending towards the HPC specialists\n",
    "\n",
    "Different buckets\n",
    "\n",
    "* General NN\n",
    "* Sequence Models - RNN/LSTM/GRU\n",
    "* Image (2d/3d), ConvNets\n",
    "* Other - Unsupervised, RL\n",
    "\n",
    "Most of what's shipping is the first 3 catagories (supervised learning), due a lot of labeled data still to be exploited.\n",
    "\n",
    "Unsupervised learning algorithms needs ever more data and computation.\n",
    "\n",
    "\n",
    "\n",
    "#### Trend: End to End DL for Complex Outputs\n",
    "\n",
    "For supervised algs\n",
    "\n",
    "Classification - output a number\n",
    "\n",
    "* Movie review -> Sentiment\n",
    "* Image -> Category\n",
    "\n",
    "Shift to more complex outputs\n",
    "\n",
    "* Audio -> text\n",
    "* Image -> caption\n",
    "* English -> French\n",
    "* Text -> Audio\n",
    "\n",
    "Achille's Heel - still need a lot of labeled data\n",
    "\n",
    "Speech Recognition - Traditional\n",
    "\n",
    "* non end to end - several intermediate steps\n",
    "* audio -> features -> phonomes -> transcript\n",
    "\n",
    "Speech Recognition - End to End\n",
    "\n",
    "* Audio -> Transcript\n",
    "\n",
    "Face recognition - turnstile\n",
    "\n",
    "* Picture of person approaching - extract face\n",
    "* Compare face to face from database\n",
    "* Not enough pictures of people approaching the turnstile to train a single end to end model\n",
    "\n",
    "Pediatrians - Look at Hand XRays to Estimate Age\n",
    "\n",
    "* Image -> Bones -> Age (works well)\n",
    "* Image -> Age (not enough data to do well)\n",
    "\n",
    "Autonomous Driving\n",
    "\n",
    "* Image -> Position of cars, Position of pestrians (DL) -> Planning Route (trad) -> steering\n",
    "* Image -> Steering (very difficult to do well based the amount of data available and the level of precision needed).\n",
    "\n",
    "Chat Bots\n",
    "\n",
    "* Best performance: text -> inference engine -> reponse\n",
    "* End to end has been done for toy problems but not suitable for production apps: text -> response\n",
    "\n",
    "### ML Strategy\n",
    "\n",
    "Example: Build Human Level Speech Recognition System\n",
    "\n",
    "Common Practice - split the data set, e.g. 60% train, 20% dev set, 20% (final) test set\n",
    "\n",
    "Common Diagnostics\n",
    "\n",
    "* Human level error - for example 1%\n",
    "* Training set error - for example 5%\n",
    "* Dev set error - for example 6%\n",
    "\n",
    "Insights\n",
    "\n",
    "* Immediately training set/human perf difference  (5% - 1%), no hope of meeting human level performance (avoidable bias)\n",
    "* Diff between dev set error and training set error indicates we're no fitting the training set that well (variance)\n",
    "\n",
    "* If we saw 1%, 2%, 6% -> over fitting\n",
    "* If we saw 1%, 6%, 10% -> high bias and high variance\n",
    "\n",
    "Recipe for Driving Machine Learning Progress\n",
    "\n",
    "1. Training error high?\n",
    "    * Yes -> Bias Problem\n",
    "        * Traing a bigger network\n",
    "        * Train longer/optimize\n",
    "        * Try new architecture\n",
    "2. Once 1 is fixed, Is Dev Set Error High?\n",
    "    * Yes -> Variance Problem\n",
    "        * More data\n",
    "        * Regularization\n",
    "        * New model architecture\n",
    "3. Done (assuming you also do well on the test set)\n",
    "\n",
    "Trend: less consideration of the bias/variance tradeoff\n",
    "\n",
    "* With deep learning we have techniques to address bias and variance independently; not always a trade off.\n",
    "\n",
    "    Data Synthesis\n",
    "\n",
    "    * OCR - Print a word on a random background, use a training set\n",
    "        * Need to do a lot of finiky work to make this work, e.g. color distribution needs to match between training and test data\n",
    "        * Need to figure out tricks to get this to work well\n",
    "    * Speech Recognition - take audio and add in background noise\n",
    "        * 10,000 hours speech, 10 hours of background added - neural network overfits to the background noise\n",
    "            * Need more background noise to randomly select (e.g. 100h or 1000h)\n",
    "    * Are we trading hand engineered features for hand engineered data sets\n",
    "    * Autonomous driving - use grand theft auto?\n",
    "        * Game probably has just 20 different types of cars\n",
    "        * Algorithm will overfit\n",
    "\n",
    "Best Practice - Unified Data Warehouse\n",
    "\n",
    "* All your customer data flows into one place\n",
    "* Much more efficient way to get things done, will move faster\n",
    "    * Data is like dynamite, need to put a lot of it together to get a big bang!\n",
    "\n",
    "Training and Test Set Distributions\n",
    "\n",
    "* Speech enabled rearview mirror\n",
    "    * 50K hours general speech recognition, not recorded from a rear view mirror in a car\n",
    "    * Add 10 hours of rearview mirror recorded in a car with a background noise, etc.\n",
    "* Approaches\n",
    "    * Train on most of the 50k, save rest of 50K for dev, test on the car data\n",
    "        * No good; development and test sets drawn from different distributions\n",
    "    * Train on the 50k set, split the in-car and use half for dev set, half for test set\n",
    "        * Dev and test drawn from the same distribution\n",
    "            * Human level - 1% \n",
    "            * Training - 10% (training - human): avoidable bias\n",
    "            * Training Dev - 10% (training dev - training): variance\n",
    "            * Dev - 10% (Dev - Training Dev): data mismatch\n",
    "            * Test - 10% (Test - Dev): Overfit dev set\n",
    "            \n",
    "Back to the recipe...\n",
    "\n",
    "1. Training error high?\n",
    "    * Yes -> Bias Problem\n",
    "        * Traing a bigger network\n",
    "        * Train longer/optimize\n",
    "        * Try new architecture\n",
    "2. Once 1 is fixed, Is Train Dev Set Error High?\n",
    "    * Yes -> Variance Problem\n",
    "        * More data\n",
    "        * Regularization\n",
    "        * New model architecture\n",
    "3. Once 2 is good, is the Dev set error high?\n",
    "    * Yes -> Data mismatch problem\n",
    "        * Make the data more similar\n",
    "        * Play with data synthesis\n",
    "        * Domain adaptation (research problem)\n",
    "        * New model architecture\n",
    "4. Once doing well on 3, Test set error is high?\n",
    "    * Yes -> Need more dev data (you've overfit the dev set)\n",
    "5. If 4 good, done.\n",
    "\n",
    "Take away: be methodical, we are learning to organize the work of machine learning.\n",
    "\n",
    "Three things you can ask?\n",
    "\n",
    "* What is human level performance?\n",
    "* Performance on examples you've trained on\n",
    "* Performance on unseen examples?\n",
    "\n",
    "Machine learning is more iterative today\n",
    "\n",
    "* Train and dev set needed to drive fast iterations\n",
    "* Dev and test with same distribution may be overfit if you can't randomly draw samples (e.g. driving data collection from 4 cities, wanting to generalize to 5th city. Here you basically have 4 data points even if you grab 1000 hours in each city)\n",
    "\n",
    "Human Level Performance\n",
    "\n",
    "* As machine learning approaches human level performance, progress slows down. Two reasons for this\n",
    "    1. Human level performance - proxy for Bayes optimal error (mathematically optimal error rate anyone can get)\n",
    "    2. Use humans to label data, use human insight in error analysis, estimate bias and variance\n",
    "    \n",
    "Future of AI\n",
    "\n",
    "* Value of supervised learning has taken off\n",
    "* Unsupervised learning is making steady progress\n",
    "* Re-enforcement learning should take off...\n",
    "* Should see transfer learning value getting big lift soon\n",
    "\n",
    "AI Product Management\n",
    "\n",
    "* Workflow is User -> PM -> Engineers\n",
    "    * PM can often mock up the app\n",
    "    * ML is different\n",
    "* Speech recognition - what to focus on?\n",
    "    * Noisy environments (car, cafe), Low bandwidth audio, accented speech, latency, binary size?\n",
    "    * PM decides\n",
    "* Fuzzy set - what customers do, another set - what ML can do. Intersection is what is possible...\n",
    "* How does the PM communicate to the engineers?\n",
    "    * PM provides dev and test sets\n",
    "    * Provides a validation metric\n",
    "* Engineering team\n",
    "    * Obtain the training data\n",
    "    * Build system that works on the dev and test set\n",
    "    \n",
    "To excel:\n",
    "\n",
    "* Read a lot of papers\n",
    "* Replicate others work\n",
    "* Dirty work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab - Regression with SKLearn Nueral Net\n",
    "\n",
    "[This notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/tree/master/DeepLearning/BikeSharingRegression)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Data set from Kaggle\n",
    "* Model predices log(rentals)\n",
    "* One hot encode categorial features\n",
    "* Scale numerical features\n",
    "* Use relu for the activation\n",
    "* Easy to use this to build a single layer network, but framework does not support GPU\n",
    "\n",
    "\n",
    "Use the column transformer \n",
    "\n",
    "* Tuple of transformer and columns to transform\n",
    "* Fit the transformer to the training data\n",
    "    * Reuse the transformer instance of all data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab - Regression with Keras and TensorFlow\n",
    "\n",
    "Lots of libraries - TensorFlow, theano, MxNet\n",
    "\n",
    "\n",
    "Keras API\n",
    "\n",
    "* High level wrapper for TensorFlow, CNTK, Theano.\n",
    "* Single API, pick your backend\n",
    "\n",
    "Lab - use [this notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/DeepLearning/BikeSharingRegression/bikerental_neural_network_keras.ipynb)\n",
    "\n",
    "* Pick the kernel you want to use corresponding to the backend you want (SageMaker)\n",
    "* Dense layer - fully conntected\n",
    "* Adam optimizer - variant of gradient descent\n",
    "* Use early stopping to avoid overfitting\n",
    "* Can rerun the notebook with a different kernel, could even take advantage of GPUs based on the underlying kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab - Binary Classification with NN\n",
    "\n",
    "Customer Churn Prediction\n",
    "\n",
    "Data prep notebook [here](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/DeepLearning/CustomerChurnClassification/customer_churn_data_preparation_onehotencoded.ipynb)\n",
    "\n",
    "NN notebook [here](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/DeepLearning/CustomerChurnClassification/customer_churn_neural_network.ipynb)\n",
    "\n",
    "Notes - Data prep:\n",
    "\n",
    "* Data prep needs to one-hot encode categorical data\n",
    "* Scale numeric features\n",
    "* Split into training, validation, test - use a validation dataset for tuning so we don't memorize the test set\n",
    "\n",
    "Notes: Model training\n",
    "\n",
    "* Metrics\n",
    "    * high precision - indicates whenever a model makes a positive prediction there is a high likelyhood of it matching ground truth.\n",
    "    \n",
    "Improving recall - lower the threshold.\n",
    "\n",
    "    * Can in some cases come up with a formula to calculate cost of misclassification and optimize the cutoff threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
