{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Deep learning - Linear Regression\n",
    "\n",
    "Set of input features and weights used to calculated a real number output\n",
    "\n",
    "* $ x_0 $ is the intercept with an initial value of 1\n",
    "* $ y = w_0 x_0 + w_1 x_1 + ... + w_m x_m = \\sum_{i=0}^N w_i X_i$\n",
    "\n",
    "Developing the model is the matter of assigning the weights to the features.\n",
    "\n",
    "* [Example notebook](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/GradientDescent/linear_cost_example.ipynb)\n",
    "\n",
    "Notes based on the notebook.\n",
    "\n",
    "* Simple data set - one feature, target is a line plus noise\n",
    "* Create a data set with some random noise - straight line with noice\n",
    "* Fit with linear regression\n",
    "\n",
    "How did the algorithm determine the weights?\n",
    "\n",
    "* Need a way to measure how close a predicton is to ground truth\n",
    "    * Squared loss error function (aka mean squared error)\n",
    "    * Loss function measures how close the predicted value is to ground truth\n",
    "    \n",
    "* Gradient descent optimizer used to determine the weights\n",
    "    * Plot the loss at different weight - plot is parabolic \n",
    "    * Algorithm starts with random wights\n",
    "    * Gradient (slope) of curve lets us know which way to go (larger or smaller) to increase or decrease the loss\n",
    "    * Negative slope - increasing weight moves downhill\n",
    "    * Positive slope - decreasing weight moves downhill\n",
    "    \n",
    "* Magnitude of weight adjustments\n",
    "    * Learning rate determines the size of the weight adjustment, tradeoff is number of iterations vs ability to converge on the optimal weight\n",
    "    * [More info on optimizing gradient descent](https://ruder.io/optimizing-gradient-descent/)\n",
    "    * Some adjust learning rate based on degree of slope, some use momentum, etc.\n",
    "    \n",
    "Gradient descent modes\n",
    "\n",
    "* Batch\n",
    "    * Compute loss for all training examples\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set. For each iteration, weight is adjusted once\n",
    "* Stochastic\n",
    "    * Compute loss for next example\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set. For each iteration, weight is adjusted 150 times\n",
    "* Mini-batch\n",
    "    * Compute loss for a specified number of examples\n",
    "    * Adjust weight\n",
    "    * Example: 150 samples in training set, mini batch size is 15. For each iteration, weight is adjusted 10 times.\n",
    "\n",
    "## Logistic Regression (Binary Classification)\n",
    "\n",
    "Set up is similar to linear regression - we have a set of features and assign a weight to each feature, we sum the products of features and weights. But, for output we want to know probability of the output belonging to the positive class, based on an output of 0 or 1.\n",
    "\n",
    "We can use the sigmoid function to run the output of the sum through - sigmoid function output is bounded between zero and one - see [this](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/GradientDescent/logistic_cost_example.ipynb) notebook.\n",
    "\n",
    "* Typically need to assign a cut off value for the output, for example anything greater than 0.5 is positive.\n",
    "\n",
    "Training objective with logistic regression is to select the weights that lowers the misclassification.\n",
    "\n",
    "* Use the logistic cost loss function, which separates negative and positive values (loss curves for positive and negative samples)\n",
    "* Logistic loss function is parabolic in nature, also with the property of not only indicating the loss at a given weight, but also indicating which direction to adjust the weight.\n",
    "\n",
    "How to find the optimal weights?\n",
    "\n",
    "* Use the gradient descent optimizer\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "Linear models are simple and easy to understand, but typically underperform on non linear data (underfit). They require extensive feature engineering, features need to be on similar range and scale.\n",
    "\n",
    "Linear models form the foundation for understanding neural networks. NN looks like stacking several logistic models, generalizing sigmoid with an activation function.\n",
    "\n",
    "Summation of features plus weights ran through an activation function is a 'neuron', at each layer the features can be connected to multiple neurons with the weights specific to each neuron.\n",
    "\n",
    "* The neurons generate new features by combining existing ones, which are then inputs to the next layer of neurons.\n",
    "* Basic architecture has an input layer, one or more hidden layers, and an output layer.\n",
    "\n",
    "Benefits\n",
    "\n",
    "* Automatic feature engineering - mixes features to create new ones\n",
    "* Handles non-linear datasets\n",
    "* Standard techniques to deal with overfitting  (easy to overfit) - regularization, reduce model complexity, etc.\n",
    "\n",
    "Activation Functions\n",
    "\n",
    "* Introduce non-linearity into the model\n",
    "* Improves ability of model to fit complex non-linear datasets\n",
    "* Three popular activation fucntions: sigmoid, tanh, relu\n",
    "\n",
    "Activation function notebook - see [here](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/GradientDescent/activation_functions.ipynb)\n",
    "\n",
    "* sigmoid - converts input to a number between 0 and 1\n",
    "* tanh - output varies from -1 to 1\n",
    "* relu - netgative input output is 0, otherwise same as input\n",
    "\n",
    "Deep learning - subset of machine learning that uses complex networks that have hundreds of layers. Why so popular?\n",
    "\n",
    "* Traditional ML algorithms appear to saturate on how much they can learn. Having massive amounts of data does not translate to more learning\n",
    "* Small NN can learn better. Medium NN can learn even more, and large NNs can keep learning with more data.\n",
    "\n",
    "Binary classifier - send the output through a sigmoid function.\n",
    "\n",
    "Multiclass classifier - use softmax to convert to array of probability scores for each class, sum of probs for all classes is 1.\n",
    "\n",
    "Popular NN architectures\n",
    "\n",
    "* General purpose\n",
    "    * fully connected network\n",
    "    * example: treats each pixel as a separate feature\n",
    "* Convolutional Neural Network (CNN)\n",
    "    * Useful for image analysis\n",
    "    * Example: considers pixels and its surrounding pixels\n",
    "* Recurrent NN\n",
    "    * Looks at history\n",
    "    * Used for timeseries prediction, natural language processing\n",
    "    * Example: timeseries forcasting - model looks at current values and historical values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIT Introduction to Deep Learning - Lecture 1\n",
    "\n",
    "Slides [here](http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L1.pdf)\n",
    "Video [here](https://www.youtube.com/watch?v=5v1JnYv_yWs&feature=youtu.be)\n",
    "\n",
    "### Why deep learning?\n",
    "\n",
    "* Traditional ML algorithms - hand engineer features, which is time consuming, brittle, not scalable\n",
    "* Can we learn the features directly from raw data?\n",
    "    * lines and edges to eyes and noses to faces\n",
    "* Why now?\n",
    "    * big data (more and larger data sets, easier collection and storage), hardware (GPUs, parallel processing), software (improved techniques, new models, open source)\n",
    "    \n",
    "### The Perceptron - the structural block of deep learning\n",
    "\n",
    "Feed-forward: inputs, weights, sum, non-linearity, output\n",
    "\n",
    "$ \\hat y = g(\\sum_{i=1}^m x_i w_i)$ where $ g $ is the activation function\n",
    "\n",
    "Note we also have another term, the bias term, which lets us shift the activation left or right:\n",
    "\n",
    "$ \\hat y = g(w_0 + \\sum_{i=1}^m x_i w_i)$ where $ w_0 $ is the bias term.\n",
    "\n",
    "We can rewrite this linear algebra style:\n",
    "\n",
    "$ \\hat y = g( w_0 + X^TW) $ where:\n",
    "\n",
    "$ X = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_m \\end{bmatrix} $ and $ W = \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_m \\end{bmatrix} $\n",
    "\n",
    "\n",
    "Activation function - typically a non-linear function like the sigmoid function, e.g.\n",
    "\n",
    "$ \\sigma (z) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-z} }  $\n",
    "\n",
    "Can also be tanh or ReLU\n",
    "\n",
    "\n",
    "### Importance of Activation Functions\n",
    "\n",
    "The purpose is to introduce non-linearities into the network, as many problems in the real world involve non-linearities.\n",
    "    * Think about nonlinear decision boundaries\n",
    "    \n",
    "    \n",
    "### Building Neural Networks with Perceptrons\n",
    "\n",
    "3 steps to computing the output of a perceptron: dot product, add a bias, take a non-linearity\n",
    "\n",
    "$ y = g(z) $ where $ z = w_0 + \\sum_{j=1}^m x_j w_j $\n",
    "\n",
    "Multi Output Perceptron\n",
    "\n",
    "$ y_1 = g(z_1) $\n",
    "$ y_2 = g(z_2) $\n",
    "$ z_i = w_0,_i + \\sum_{j=1}^m x_j w_j,_i $\n",
    "\n",
    "Single Layer Nueral Network\n",
    "\n",
    "* Input layer, fully connected hidden layer, output layer (two outputs)\n",
    "\n",
    "$ z_i = w^{(1)}_{0,i} + \\sum_{j=1}^m x_j w^{(1)}_{j,i} $\n",
    "\n",
    "and...\n",
    "\n",
    "$ \\hat {y_i} = g(w^{(2)}_{0,i} + \\sum_{j=1}^{d_1} z_j w^{(2)}_{0,i})$\n",
    "\n",
    "Middle layer has $ z_1 ... z_{d_1} $ nodes\n",
    "\n",
    "The hidden layer is learning, not observable like the input layer and the output layer.\n",
    "\n",
    "Another name for fully connected layers is a dense layer.\n",
    "\n",
    "In Keras/TF:\n",
    "\n",
    "```\n",
    "from tf.keras.layers import *\n",
    "\n",
    "inputs = Inputs(m)\n",
    "hidden = Dense(d1)(inputs)\n",
    "outputs = Dense(2)(hidden)\n",
    "model = Model(inputs, outputs)\n",
    "```\n",
    "\n",
    "\n",
    "### Applying Neural Networks\n",
    "\n",
    "Will I pass this class?\n",
    "\n",
    "* Two inputs: hours spend on the final project, number of lectures attended\n",
    "* Output: pass/fail\n",
    "\n",
    "How to train the model? First need to know how to quantify the loss.\n",
    "\n",
    "$ L(f(x^{(i)};W), y^{(i)}) $ (compares predicted and actual value)\n",
    "\n",
    "Loss is low if close to actual, higher if not.\n",
    "\n",
    "Empiracal loss - measures the total over out entire dataset.\n",
    "\n",
    "* aka Objective function, cost function, empirical risk\n",
    "\n",
    "$ J(W) = \\frac{1}{n} \\sum_{i=1}^n L(f(x^{(i)};W), y^{(i)})$\n",
    "\n",
    "0/1 Output - use Binary Cross Entropy Loss\n",
    "Computer a grade or number output - use Mean Squared Error Loss\n",
    "\n",
    "### Training Neural Networks\n",
    "\n",
    "Training is about loss optimization. We want to find the network weights that achive the lowest loss\n",
    "\n",
    "$ W^{*} = \\frac{argmin}{W} \\frac{1}{n} \\sum_{i=1}^n L(f(x^{(i)};W), y^{(i)}) $\n",
    "\n",
    "$ W^{*} = \\frac{argmin}{W} J(W) $\n",
    "\n",
    "Remember: $ W = \\{ W^{(0)},W^{(1)},... \\} $\n",
    "\n",
    "We find the optimal weights via Gradient Descent\n",
    "\n",
    "Algoritm:\n",
    "\n",
    "1. Initiaize weights randomly $ \\sim N(O, \\sigma^2 )$\n",
    "2. Loop until convergence\n",
    "\n",
    "    1. Compute gradient $ \\frac{\\partial{J(W)}}{\\partial{W}} $\n",
    "    2. Update weights $ W \\leftarrow W - \\eta \\frac{\\partial{J(W)}}{\\partial{W}} $\n",
    "5. Return weights\n",
    "\n",
    "```\n",
    "weights = tf.random_nomal(shape, stddev=sigma)\n",
    "grads = tf.gradients(ys=loss,xs=weights)\n",
    "weights_new=weights.assign(weights = lr * grads)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the Gradients: Backpropagation\n",
    "\n",
    "Single network: $x \\rightarrow w_1 \\rightarrow z_1 \\rightarrow w_2 \\rightarrow \\hat y \\rightarrow J(W)$\n",
    "\n",
    "How does a small change in weight ($w_2$) affect the final loss $ J(W) $?\n",
    "\n",
    "Unpack using the chain rule\n",
    "\n",
    "$ \\frac{\\partial{J(W)}}{\\partial{w_2}} = \\frac{\\partial{J(W)}}{\\partial{\\hat y}} * \\frac{\\partial{\\hat y}}{\\partial{w_2}}$\n",
    "\n",
    "And the influence of w1? Apply the chain rule again\n",
    "\n",
    "$ \\frac{\\partial{J(W)}}{\\partial{w_1}} = \\frac{\\partial{J(W)}}{\\partial{\\hat y}} * \\frac{\\partial{\\hat y}}{\\partial{z_1}} * \\frac{\\partial{z_1}}{\\partial{w_1}}$\n",
    "\n",
    "Repeat this for every weight in the enetwork using gradients from later layers.\n",
    "\n",
    "\n",
    "### Neural Networks in Practice: Optimization\n",
    "\n",
    "In practice training neural networks is difficult. Loss landscape is complex with many local optima. Loss optimization can be difficult to optimize.\n",
    "\n",
    "Setting the learning rate $ \\eta $ is difficult and can greatly affect the optimization.\n",
    "\n",
    "How to deal with this?\n",
    "\n",
    "* Try a lot of learning rates to see what works best.\n",
    "* Adaptive learning rates - don't fix the rate, but adjustis based on how large the gradient is, how fast learning is happenig, zied of particular weights, etc.\n",
    "    * Tensor flow examples: momentum, adagrad, adadelta, adam, rmsprop\n",
    "    \n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "Just compute at a single point to save some cycles... but noisy\n",
    "\n",
    "1. Initiaize weights randomly $ \\sim N(O, \\sigma^2 )$\n",
    "2. Loop until convergence\n",
    "    1. Pick single data point $i$\n",
    "    1. Compute gradient $ \\frac{\\partial{J_i(W)}}{\\partial{W}} $\n",
    "    2. Update weights $ W \\leftarrow W - \\eta \\frac{\\partial{J(W)}}{\\partial{W}} $\n",
    "5. Return weights\n",
    "\n",
    "### Mini-batches\n",
    "\n",
    "Easier to compute, less noise than stochastic as you are considering a wider population\n",
    "\n",
    "1. Initiaize weights randomly $ \\sim N(O, \\sigma^2 )$\n",
    "2. Loop until convergence\n",
    "    1. Pick batch of B data points\n",
    "    1. Compute gradient $ \\frac{\\partial{J_i(W)}}{\\partial{W}} = \\frac{1}{B}\\sum_{k=1}^B \\frac{\\partial{J_k(W)}}{\\partial{W}} $\n",
    "    2. Update weights $ W \\leftarrow W - \\eta \\frac{\\partial{J(W)}}{\\partial{W}} $\n",
    "5. Return weights\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Want a model that performs well and generalizes well\n",
    "\n",
    "* Underfit - model not complex enough to fully learn\n",
    "* Ideal fit\n",
    "* Overfitting - too complex, extra parameters, memorizes the training set, does not generalize well\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "Regularization is a technique that constrains our optimization problem to discourage complex models. Why do we need it? To improve generalization of our model to unseen model.\n",
    "\n",
    "Regularization 1: Drop out\n",
    "\n",
    "* During training, randomly set some activations to 0\n",
    "    * Typically drop 50% of activations in a layer in any given training iteration\n",
    "    * Creates an ensemble of multiple models through the paths\n",
    "    * Forces network to not rely on any 1 node\n",
    "    \n",
    "Regularization 2: Early stopping\n",
    "\n",
    "* Stop training before we have a change to overfit.\n",
    "    * Usually prior to where the loss of the testing performance starts rising\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIT 6.S191 (2019): Convolutional Neural Networks\n",
    "\n",
    "Lecture 3 from Introduction to Deep Learning\n",
    "\n",
    "Video [here](https://www.youtube.com/watch?v=H-HVZJ7kGI0&feature=youtu.be)\n",
    "Slides [here](http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L3.pdf)\n",
    "\n",
    "Image recognition\n",
    "\n",
    "* Input is a 2D image, vector of pixel values\n",
    "* Output - class label, cna produce probability of belonging to a particular class\n",
    "\n",
    "Fully connected network architecture\n",
    "\n",
    "* Not a good fit for vision processing\n",
    "* Squashing a 2D matrix into a vector and fully connecting it means you lost spatial information, and connectivity makes computation too expensive\n",
    "\n",
    "Goal: use an architecture that uses the spatial structure\n",
    "\n",
    "* connect patches of input to neurons in hidden layer\n",
    "* Pixels next to each other are probably realted\n",
    "* Use a sliding patch windows across the image to define connections\n",
    "\n",
    "How to weight to weight the patch to detect particular features?\n",
    "\n",
    "* Apply a set of weights - a filter - to extract local features.\n",
    "* Use multiple filters to extract different features\n",
    "* Spatially share parameters of each filter\n",
    "\n",
    "Feature extraction with convolution\n",
    "\n",
    "* Filter of size 4x4: 16 different weights\n",
    "* Apply this same filter to 4x4 patches in the input\n",
    "* Shift by 2 pixels for next patch\n",
    "\n",
    "Convolution\n",
    "\n",
    "* Uses filters to identify where features expressed in filter 'pop up' in the image\n",
    "* Perform element wise multiplication of patch with location of window on image\n",
    "\n",
    "Example - 5x5 image convolved with a 3x3 filter\n",
    "\n",
    "* Yields 3x3 feature map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
