{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "A note on terminology, from the course notes.\n",
    "\n",
    "> In the lectures, particularly PCA ones, we had to bring all features to a similar scale before applying PCA transformation. I referred to the data scaling transformation operation as Normalization.\n",
    ">\n",
    "> This was based on the definition given in [AWS materials](https://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html#normalization-transformation): \"The normalization transformer normalizes numeric variables to have a mean of zero and variance of one.\"\n",
    ">\n",
    "> However, [SKLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and the practitioners refer to this as Standardization, e.g. \"Standardize features by removing the mean and scaling to unit variance\".\n",
    ">\n",
    "> So, the operation I am performing in the lectures is indeed Standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "From the SageMaker docs:\n",
    "\n",
    ">PCA is an unsupervised machine learning algorithm that attempts to reduce the dimensionality (number of features) within a dataset while still retaining as much information as possible. This is done by finding a new set of features called components, which are composites of the original features that are uncorrelated with one another. They are also constrained so that the first component accounts for the largest possible variability in the data, the second component the second most variability, and so on.\n",
    "\n",
    "Dimensionality reduction - retain information while reducing features\n",
    "\n",
    "New PCA features may be important predictors of the target, but... how do you map the importance of the component to real world features?\n",
    "\n",
    "Constraints\n",
    "\n",
    "* Works only for numeric data\n",
    "* Data needs to be normalized to features with similar scale\n",
    "* For large dimension datasets, PCA is an option to reduce features and use that for training a model\n",
    "\n",
    "Number of components\n",
    "\n",
    "* Libraries typically let you specify the number of components to reduce to\n",
    "* Total variation to capture as a percentage (e.g. capture 90% of the info), in this case PCA figures out the number of components.\n",
    "\n",
    "PCA on SageMaker\n",
    "\n",
    "* Regular mode - good on sparse and moderate size datasets\n",
    "* Random - good for very large datasets, uses approximation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demos\n",
    "\n",
    "Random dataset demo - see [here](https://github.com/ChandraLingam/AmazonSageMakerCourse/blob/master/pca/ExplorePCA/random_data_preparation.ipynb)\n",
    "\n",
    "* StandardScaler is used to normalize the dataset\n",
    "* PCA modeule\n",
    "* Both are from sklearn\n",
    "* Generate a random dataset - verify little correlation\n",
    "* To initialize PCA, you can specify the number of components, or a fraction representing the percent variation to capture.\n",
    "* PCA can't do much with the random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
