{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to SageMaker\n",
    "\n",
    "Fully managed cloud based machine learning service, made up of 3 capabilities\n",
    "\n",
    "* Build - Jupyter Notebook development environment\n",
    "    * Extensive collection of popular machine learning algorithms\n",
    "    * Preconfigured to run TensorFlow and Apache MxNet\n",
    "    * Bring your own algorithm\n",
    "* Train - managed training infrastructure\n",
    "    * Distribute training across one or more instances\n",
    "    * Managed model training infrastructure\n",
    "    * Scales to petabytes\n",
    "    * Compute instances automatically launched and release, artifacts stored in S2\n",
    "* Deploy - scalable hosting infrastructure\n",
    "    * Real time prediction\n",
    "        * For interactive and low latency used cases\n",
    "        * Autoscaling to maintain adequate capacity, replace unhealthy instances, scale-out and scale-in based on workload\n",
    "    * Batch transform\n",
    "        * Non-interactive use-cases\n",
    "        * Suitabke where you need inference for your entire dataset, don't need a persistent real-time endpoint, don't need sub-second latency performance\n",
    "        * Manages all resources needed for batch transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Types and Pricing\n",
    "\n",
    "Instance families\n",
    "\n",
    "* Standard\n",
    "    * balanced CPU, memory, and network\n",
    "    * T2, T3, M5 - T for bursty, M can handle sustained load\n",
    "* Compute optimized\n",
    "    * Highest CPU perf\n",
    "    * Latest CPUs - C4, C5\n",
    "    * Good for both training and hosting\n",
    "* Accelerated computing\n",
    "    * graphics/GPU compute\n",
    "    * Speed up algs optimized for GPUs\n",
    "    * P2, P3\n",
    "    * Costs more, but can reduce training time, can also serve GPU optimized inferencce\n",
    "* Inference acceleration\n",
    "    * Add-on Fractional GPUs\n",
    "    * Some algorithms are GPU intensive during training but need only fractional GPU during inference\n",
    "    \n",
    "    \n",
    "How to decide?\n",
    "\n",
    "* CPU vs GPU\n",
    "* Try difference sizes when family selected\n",
    "* Instance type and size\n",
    "    * <instance type><hardware gen>.<size> e.g. c5.2xlarge\n",
    "    \n",
    "Pricing components\n",
    "\n",
    "* Instance type and size\n",
    "* Fractional GPUs\n",
    "* Storage\n",
    "* Data transfer\n",
    "* Region\n",
    "\n",
    "Training - On Demand Pricing\n",
    "\n",
    "* Instance hourly cost\n",
    "* Storage\n",
    "* Instances are automatically launched and terminated\n",
    "\n",
    "Hosting - Realtime\n",
    "\n",
    "* Instance + Fractional GPU \n",
    "* Storage\n",
    "* Data transfer\n",
    "\n",
    "Hosting - Batch\n",
    "\n",
    "* Instance + Fractional GPU \n",
    "* Storage\n",
    "* Data transfer\n",
    "* Automatic termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
