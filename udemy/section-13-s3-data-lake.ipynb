{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Lakes in AWS\n",
    "\n",
    "## Overview\n",
    "\n",
    "Typical ML project - spends 70% of the effort on data prep and cleaning. A data lake can help streamline the data related parts of an ML project.\n",
    "\n",
    "Data Lake vs Data Warehouse\n",
    "\n",
    "* A data lake is a vast pool of raw data, the purpose for which is not yet defined. A data warehouse is a repository for structured, filtered data that has already been processes for a specific purpose.\n",
    "\n",
    "AWS whitepaper - [Data Lake on AWS](https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html)\n",
    "\n",
    "Aspects of data lakes\n",
    "\n",
    "* Storage \n",
    "* Governance\n",
    "* Analytics\n",
    "\n",
    "AWS data lake related services\n",
    "\n",
    "* Storage\n",
    "    * S3 - storage\n",
    "    * Glacier - backup and archival\n",
    "    \n",
    "* Ingestion\n",
    "    * Kinesis Firehose\n",
    "    * Storage Gateway - for on premise data to cloud. Three modes - file share, block storage, or virtual tape device.\n",
    "    * Large data migration - Snowball appliance (petabyte scale transfers) and Snowmobile (exabyte scale).\n",
    "    * SDK, CLI and more to store data in S3\n",
    "    \n",
    "* Data Catalog\n",
    "    * Without a metadata catalog to aid in discover of data in the data lake you end up with a data swamp.\n",
    "    * A data swamp is a deteriorated and unmanaged data lake that is either inaccessible to its intended users or is providing little value.\n",
    "    * DIY - make data discoverable and usable. Build your own using s3, lambda, elastic search, dynamodb to maintain metadata.\n",
    "    * Glue - data catalog (metadata repository). Automatically crawl and collect metadata from s3, ddb, and any database that supports jdbc connectivity,\n",
    "    \n",
    "## Kinesis\n",
    "\n",
    "Allows you to ingest, buffer, and process streaming data in real-time.\n",
    "\n",
    "Streaming data\n",
    "\n",
    "* Generated continuously from thousands of sources. \n",
    "* Small payloads\n",
    "\n",
    "Batch processing - data ingested and stored, processed in batches at various points in time (hourly, daily, etc).\n",
    "Stream processing - analyze data as it arrives, response in seconds.\n",
    "\n",
    "Kinesis Capabilities\n",
    "\n",
    "* Video streams - use for video playback, monitoring, rekognition, etc\n",
    "* Data streams - data streaming, use kinesis data analytics, spark on EMR, EC2, lambda\n",
    "* Firehose - collect data and directly load in the s3, redshift, elasticsearch, and splunk.\n",
    "* Kinesis Data Analytics - process using SQL or Flink\n",
    "\n",
    "## Data Formats and Tools\n",
    "\n",
    "Optimal format can lower storage cost, improve query performance.\n",
    "\n",
    "One of the core values of a data lake is that it is the collection point and repository for all of an organization's data assetes, in whatever their native formats are.\n",
    "\n",
    "Recommendation:\n",
    "\n",
    "* Collect in native format\n",
    "* Transform data in data lake\n",
    "\n",
    "Data Organization\n",
    "\n",
    "* Row storage - optimized for reading entire row\n",
    "* Column storage - optimized for read subset of columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formats - Text\n",
    "\n",
    "\n",
    "| Format | Organization | Use |\n",
    "| --- | --------- | :------------ |\n",
    "| csv, tsv | row | easy to use, no data type support, duplication when used for hierarchical, data not optimized for reading specific columns|\n",
    "| json | row | format of choice for comm. between web services, supports data types, efficiently represent hierarchical data |\n",
    "| json lines | row | new line delimited json, convenient for processing one record at a time |\n",
    "\n",
    "### Data Formats - Binary\n",
    "\n",
    "| Format | Organization | Use |\n",
    "| --- | --------- | :------------ |\n",
    "| Parquet | Columnar | ideal for use cases that require only subset of columns, efficiently query large amounts of data, WORM, compressed storage, extensive tool support, data type support |\n",
    "| ORC | Columnar | optimized row columnar storage, like parquet |\n",
    "| avro | row | ideal for write-heavy use cases, ideal for scenarios where you need to read the entire record, data type support |\n",
    "\n",
    "Example of columnar storage [here](https://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html). \n",
    "\n",
    "### Data Transformation\n",
    "\n",
    "Recommended Approach\n",
    "\n",
    "* Collect in native format\n",
    "* Transform in data lake\n",
    "\n",
    "Approaches\n",
    "\n",
    "| Service | Purpose | Use |\n",
    "| --- | --------- | :------------ |\n",
    "| Amazon EMR | big data prep and processing | managed hadoop environment, support for tools like spark, hive, hbase, support for ml tools like tensorflow and mxnet |\n",
    "| Glue | ETL | automatically generate etl scripts, schedule and run on managed spark environment |\n",
    "| Kinesis Firehose | streaming data transformation | transform streaming data to parquet, orc, deliver transformed data to aws data stores, backup original data to s3 |\n",
    "\n",
    "\n",
    "EMR format conversion approaches\n",
    "\n",
    "* source format to s3 -> load to hive tables -> export to target format\n",
    "* source format to s3 -> spark -> transform data to desired format\n",
    "* point glue to source and target, glue generates the scripts, creates the spark cluster, runs the job etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Place Analytics and Portfolio of Tools\n",
    "\n",
    "\n",
    "A couple options:\n",
    "\n",
    "* Athena -> SQL -> S3\n",
    "* Redshift spectrum -> SQL -> S3\n",
    "\n",
    "| Service | Purpose | Use |\n",
    "| --- | --------- | :------------ |\n",
    "| Athena | In-place SQL query | Query data in s3 without needed to ETL into separate service or platform, charged based on amount of data scanned |\n",
    "| Redshift Spectrum | In-place SQL query (redshift compatible SQL) | Query data in s3 without needed to ETL into separate service or platform, more suitable for complex queries and large datasets (up to exabytes)|\n",
    "\n",
    "Recommendations\n",
    "\n",
    "* Athena - ad hoc queries, discovery\n",
    "* Redshift - more complex queries, large numbers of users\n",
    "\n",
    "### Streaming Query\n",
    "\n",
    "| Service | Purpose | Use |\n",
    "| --- | --------- | :------------ |\n",
    "| Kinesis data analytics | Streaming data SQL query | Query and analyze streaming data with SQL |\n",
    "\n",
    "### Broader  Analytics Portfolio\n",
    "\n",
    "| Service | Purpose | Use |\n",
    "| --- | --------- | :------------ |\n",
    "| Amazon EMR | Hadoop ecosystem tools | Run a variety of workloads using spark, hive, pig, hbase, tensorflow, mxnet, etc.|\n",
    "| SageMaker | Machine learning | Managed machine learning service with a variety of algoritms |\n",
    "| Artificial Intelligence | Video, Image, Natural Langauge | Pre-trainiend, ready to use AI services for video analysis, speech and natural language processing, etc |\n",
    "| Quicksight | Business intelligence | Managed BI tool to create interactive dashboards |\n",
    "| Redshift | Data warehouse (columnar storage) | Managed petabyte scale data warehouse. SQL based querying and easily integrates with your existing BI tools |\n",
    "| Lambda | Business logic | Serverless backend processing logic with trigger-based code execution |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Optimization\n",
    "\n",
    "CloudWatch\n",
    "\n",
    "* Track operational metrics\n",
    "* Set alarms\n",
    "* Use metrics to scale resources\n",
    "\n",
    "Logs\n",
    "\n",
    "* Consolidata logs\n",
    "* Scrape and alert\n",
    "\n",
    "Cloud Trail\n",
    "\n",
    "* Audit trail - who, what, when\n",
    "* Logs land in S3, can query using Athena\n",
    "\n",
    "Cost Optimization\n",
    "\n",
    "* S3 lifecycle management\n",
    "    * Migration from standard, infrequent action, glacier\n",
    "* S3 storage class analysis\n",
    "* Intelligent tiering\n",
    "* Amazon glacier and glacier deep archive\n",
    "* data formats\n",
    "\n",
    "## Security and Protection\n",
    "\n",
    "By nature data lake consilidates all data into a single place. Therefore it is important to protect the data lake from a security perspective.\n",
    "\n",
    "Can use resource based policies and user based policies\n",
    "\n",
    "Resource based policy\n",
    "\n",
    "* Attached directly to a resource such as a bucket or object\n",
    "\n",
    "\n",
    "User based policies\n",
    "\n",
    "* Permissions granted to users and groups\n",
    "\n",
    "Easier to manage permissions at the user/group level, but consider resource level policies that must be enforced for all.\n",
    "\n",
    "S3 Data Encryption\n",
    "\n",
    "* Protection of data at rest\n",
    "* Integrated with KMS\n",
    "* Use CMKs to protect data, data is protected even if bucket made public\n",
    "\n",
    "### Protection\n",
    "\n",
    " A data lake must protect data against corruption, loss, accidental or malicious overwrites, modifications, and deletions.\n",
    " \n",
    " S3\n",
    " \n",
    " * S3 11 nines durability 99.999999999% durability\n",
    " * Versioning - protect against accidental and malicious deletes\n",
    "     * Can use lifecycles rules to manage versions\n",
    "     * Can use MFA for delete protection\n",
    " * Cross-region replication\n",
    " * Object tagging for object level metadata, define access policies based on tags\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
